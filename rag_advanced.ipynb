{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Grant Glass](https://glassgrant.com) for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email grantg@unc.edu.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Large Language Models and Embeddings for Retrieval Augmented Generation: Day 3 7/19/24\n",
    "\n",
    "This is lesson `3` of 3 in the educational series on `Large Language Models (LLMs) and Retrieval Augmented Generation (RAG)`. This notebook focuses on advanced techniques for optimizing RAG systems.\n",
    "\n",
    "**Skills:** \n",
    "* Data analysis\n",
    "* Machine learning\n",
    "* Text analysis\n",
    "* Language models\n",
    "* Vector embeddings\n",
    "* Retrieval Augmented Generation\n",
    "* Performance optimization\n",
    "\n",
    "**Audience:** `Learners`\n",
    "\n",
    "**Use case:** `Tutorial`\n",
    "\n",
    "This tutorial guides users through advanced techniques for optimizing Retrieval Augmented Generation systems.\n",
    "\n",
    "\n",
    "\n",
    "**Difficulty:** `Advanced`\n",
    "\n",
    "Advanced assumes users are very familiar with Python and have been programming for years, but they may not be familiar with the specific optimization techniques for RAG systems.\n",
    "\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python programming (including object-oriented programming)\n",
    "* Understanding of LLMs and embeddings (covered in Days 1 and 2)\n",
    "* Basic knowledge of RAG systems (covered in Day 2)\n",
    "\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "* Experience with natural language processing (NLP)\n",
    "* Familiarity with information retrieval concepts\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "1. Implement advanced retrieval techniques for RAG systems\n",
    "2. Optimize prompt engineering for improved RAG performance\n",
    "3. Develop strategies for handling long contexts in RAG\n",
    "4. Implement and evaluate different reranking methods\n",
    "5. Create a more sophisticated RAG pipeline integrating multiple optimization techniques\n",
    "\n",
    "\n",
    "**Research Pipeline:**\n",
    "1. Introduction to LLMs and their applications (Day 1)\n",
    "2. Exploring embeddings and introduction to RAG (Day 2)\n",
    "3. **Optimizing RAG systems for enhanced performance**\n",
    "4. Applying optimized RAG in research contexts\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "\n",
    "* [OpenAI](https://github.com/openai/openai-python) for generating embeddings and interacting with GPT models\n",
    "* [Pandas](https://pandas.pydata.org/) for data manipulation\n",
    "* [NumPy](https://numpy.org/) for numerical operations\n",
    "* [Scikit-learn](https://scikit-learn.org/) for similarity calculations and evaluation metrics\n",
    "* [FAISS](https://github.com/facebookresearch/faiss) for efficient similarity search\n",
    "* [NLTK](https://www.nltk.org/) for text preprocessing\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install Libraries ###\n",
    "!pip install openai pandas numpy scikit-learn faiss-cpu nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "# Import the OpenAI library to interact with the OpenAI API, useful for tasks like text generation or semantic search.\n",
    "import openai\n",
    "\n",
    "# Import the OpenAI class from the openai library for direct use of its methods (though this seems redundant given the previous import)\n",
    "from openai import OpenAI\n",
    "\n",
    "# Import pandas, a powerful data manipulation and analysis library for Python.\n",
    "import pandas as pd\n",
    "\n",
    "# Import numpy, a library for numerical operations on large, multi-dimensional arrays and matrices.\n",
    "import numpy as np\n",
    "\n",
    "# Import cosine_similarity from sklearn, a method for calculating the cosine similarity between vectors, useful in various machine learning tasks.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Import precision_score, recall_score, f1_score from sklearn for evaluating the accuracy of a classification.\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Import faiss, a library for efficient similarity search and clustering of dense vectors.\n",
    "import faiss\n",
    "\n",
    "# Import nltk, a toolkit for natural language processing (NLP) tasks.\n",
    "import nltk\n",
    "\n",
    "# From nltk, import word_tokenize for splitting strings into words and stopwords for filtering out common words.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import os, a module for interacting with the operating system, useful for file paths, environment variables, etc.\n",
    "import os\n",
    "\n",
    "# The line intended to import the faiss library is misspelled. It should be \"import faiss\".\n",
    "# Faiss is a library for efficient similarity search and clustering of dense vectors.\n",
    "import faiss\n",
    "\n",
    "# Import the ast library, which is used for processing trees of the Python abstract syntax grammar.\n",
    "# The ast module helps in introspecting and analyzing Python code.\n",
    "# Add this import to handle string to list conversion\n",
    "import ast  \n",
    "\n",
    "# Download the 'punkt' tokenizer models. This is necessary for tokenizing words in text.\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download the list of stopwords to filter out common words that are usually irrelevant in NLP tasks.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedd148",
   "metadata": {},
   "source": [
    "# Required Data\n",
    "\n",
    "We'll continue using the dataset from Day 2.\n",
    "\n",
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9da404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('day2_dataset_adaptation.csv')  # Assuming we saved the DataFrame from Day 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this final lesson of our LLMs with RAG workshop, we'll focus on advanced techniques for optimizing Retrieval Augmented Generation (RAG) systems. We'll explore methods to enhance retrieval accuracy, improve prompt engineering, handle long contexts, and implement reranking strategies.\n",
    "\n",
    "Key topics we'll cover:\n",
    "1. Advanced retrieval techniques\n",
    "2. Optimizing prompt engineering\n",
    "3. Handling long contexts\n",
    "4. Implementing reranking methods\n",
    "5. Building an optimized RAG pipeline\n",
    "\n",
    "Let's begin by setting up our OpenAI API access and defining some utility functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996cb30a",
   "metadata": {},
   "source": [
    "## Configure the OpenAI client\n",
    "\n",
    "To setup the client for our use, we need to create an API key to use with our request. Skip these steps if you already have an API key for usage.\n",
    "\n",
    "You can get an API key by following these steps:\n",
    "\n",
    "1. [Create a new project](https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects)\n",
    "2. [Generate an API key in your project](https://platform.openai.com/api-keys)\n",
    "3. (RECOMMENDED, BUT NOT REQUIRED) [Setup your API key for all projects as an env var](https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d06be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1: Directly paste the API key (not recommended for production or shared code)\n",
    "client = OpenAI(api_key=\"your_actual_openai_api_key_here\")\n",
    "\n",
    "# Method 2: Use an environment variable (recommended for most use cases)\n",
    "# Ensure the environment variable OPENAI_API_KEY is set in your environment before running the script\n",
    "#client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Method 3: Use a configuration file (alternative for keeping keys out of code)\n",
    "# Create a file named config.py (or similar) and define OPENAI_API_KEY in it, then import it here\n",
    "#from config import OPENAI_API_KEY\n",
    "#client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Method 4: Use Python's built-in `getpass` module to securely input the API key at runtime (useful for notebooks or temporary scripts)\n",
    "#from getpass import getpass\n",
    "#api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "#client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the embedding of a given text.\n",
    "# This function takes a text string and an optional model name as input.\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    # Replace newline characters with spaces in the text to ensure it's in a single line.\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Use the OpenAI API client to create an embedding for the text using the specified model.\n",
    "    # The function returns the embedding of the first (and only) input text.\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "# Define a function to get a completion for a given prompt using GPT.\n",
    "# This function takes a prompt string and an optional model name as input.\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\"):\n",
    "    # Structure the prompt into a format suitable for the OpenAI API, specifying the role as \"user\".\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # Use the OpenAI API client to create a chat completion using the specified model.\n",
    "    # The temperature is set to 0 for deterministic output, meaning no randomness in the response.\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    # Return the content of the first (and only) message in the response.\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Print a message indicating that the OpenAI API is ready for use.\n",
    "print(\"OpenAI API is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0042720",
   "metadata": {},
   "source": [
    "# Lesson\n",
    "\n",
    "## 1. Advanced Retrieval Techniques\n",
    "\n",
    "Let's implement a more sophisticated retrieval system using FAISS for efficient similarity search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a24518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FAISSRetriever:\n",
    "    # Initialize the retriever with a DataFrame containing the documents and their embeddings\n",
    "    def __init__(self, df, client):\n",
    "        self.df = df  # Store the DataFrame\n",
    "        self.index = None  # Initialize the FAISS index as None\n",
    "        self.client = client  # Store the OpenAI API client\n",
    "        # Check if the DataFrame is not empty, contains an 'embeddings' column, and all embeddings are not null\n",
    "        if not df.empty and 'embeddings' in df.columns and not df['embeddings'].isnull().all():\n",
    "            self.build_index()  # Build the FAISS index\n",
    "        else:\n",
    "            # If conditions are not met, print a warning and do not build the index\n",
    "            print(\"DataFrame is empty or does not contain valid embeddings. Index will not be built.\")\n",
    "\n",
    "    # Method to build the FAISS index\n",
    "    def build_index(self):\n",
    "        try:\n",
    "            # Check if embeddings are stored as strings; if so, convert them to lists\n",
    "            if isinstance(self.df['embeddings'].iloc[0], str):\n",
    "                # Convert string embeddings to lists using ast.literal_eval and stack them vertically\n",
    "                embeddings = np.vstack([ast.literal_eval(emb) for emb in self.df['embeddings'].values])\n",
    "            else:\n",
    "                # If embeddings are not strings, stack them vertically as they are\n",
    "                embeddings = np.vstack(self.df['embeddings'].values)\n",
    "        except ValueError as e:\n",
    "            # Catch and print any ValueError that occurs during stacking\n",
    "            print(f\"Error stacking embeddings: {e}\")\n",
    "            return\n",
    "\n",
    "        # Check if the embeddings are a 2D array as expected\n",
    "        if embeddings.ndim != 2:\n",
    "            print(f\"Expected embeddings to be a 2D array, got {embeddings.ndim}D instead.\")\n",
    "            return\n",
    "\n",
    "        # Create a FAISS index for L2 distance and add the embeddings\n",
    "        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "\n",
    "    # Method to get embedding using the OpenAI API\n",
    "    def get_embedding(self, text, model=\"text-embedding-3-small\"):\n",
    "        # Replace newline characters with spaces in the text to ensure it's in a single line.\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        # Use the OpenAI API client to create an embedding for the text using the specified model.\n",
    "        # The function returns the embedding of the first (and only) input text.\n",
    "        response = self.client.embeddings.create(input=[text], model=model)\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    # Method to retrieve documents similar to a given query\n",
    "    def retrieve(self, query, k=10):\n",
    "        # Generate query embedding using the get_embedding method\n",
    "        query_embedding = np.array(self.get_embedding(query)).reshape(1, -1)\n",
    "        # Perform the search with the query embedding\n",
    "        _, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        # Filter out-of-bounds indices\n",
    "        valid_indices = [index for index in indices[0] if index < len(self.df)]\n",
    "        # Check if any indices were filtered out\n",
    "        if len(valid_indices) != len(indices[0]):\n",
    "            print(f\"Warning: {len(indices[0]) - len(valid_indices)} out-of-bounds indices were removed.\")\n",
    "        # Return the DataFrame rows corresponding to the valid indices of the retrieved documents\n",
    "        return self.df.iloc[valid_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce790d",
   "metadata": {},
   "source": [
    "## 2. Optimizing Prompt Engineering\n",
    "\n",
    "Let's create a more sophisticated prompt template that includes multiple retrieved documents and encourages the model to synthesize information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create an optimized prompt for text generation tasks.\n",
    "# This function limits the total number of tokens, the number of documents, and the length of each document.\n",
    "def create_optimized_prompt(query, retrieved_docs, max_tokens=16385, max_docs=3, max_length_per_doc=500):\n",
    "    \"\"\"\n",
    "    Create an optimized prompt with a limit on the total number of tokens, the number of documents,\n",
    "    and the length of each document.\n",
    "    \"\"\"\n",
    "    # Start building the prompt with an introduction and the query.\n",
    "    prompt = f\"\"\"Given the following query and relevant information from multiple sources, \n",
    "    provide a comprehensive and accurate answer. Synthesize the information from all sources,\n",
    "    and if there are any contradictions or gaps in the information, point them out.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Relevant Information:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a counter for the number of documents added to the prompt.\n",
    "    doc_count = 0\n",
    "    # Iterate over the retrieved documents.\n",
    "    for i, doc in retrieved_docs.iterrows():\n",
    "        # Stop adding documents if the maximum number of documents has been reached.\n",
    "        if doc_count >= max_docs:\n",
    "            break\n",
    "        # Truncate the document text to the maximum length per document.\n",
    "        doc_text = doc['fullText'][:max_length_per_doc]\n",
    "        # Add the document title and truncated text to the prompt.\n",
    "        prompt += f\"\\nSource {i+1} - {doc['title']}:\\n{doc_text}\\n\"\n",
    "        # Increment the document counter.\n",
    "        doc_count += 1\n",
    "    \n",
    "    # Append a section for the synthesized answer to the prompt.\n",
    "    prompt += \"\\nSynthesized Answer:\"\n",
    "    \n",
    "    # Check if the prompt exceeds the maximum token limit.\n",
    "    if len(prompt) > max_tokens:\n",
    "        # Truncate the prompt to fit the token limit.\n",
    "        prompt = prompt[:max_tokens]\n",
    "    \n",
    "    # Return the constructed prompt.\n",
    "    return prompt\n",
    "\n",
    "# Example usage of the function.\n",
    "retriever = FAISSRetriever(df, client)\n",
    "# Define a query.\n",
    "query = \"How do different authors explore the concept of adaptation?\"\n",
    "# Retrieve documents relevant to the query.\n",
    "retrieved = retriever.retrieve(query)  # Assume 'retriever' is a previously defined object with a 'retrieve' method.\n",
    "# Create an optimized prompt using the retrieved documents.\n",
    "optimized_prompt = create_optimized_prompt(query, retrieved)\n",
    "# Generate a response using the optimized prompt.\n",
    "response = get_completion(optimized_prompt)\n",
    "# Print the query and the generated response.\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Optimized RAG Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cce476",
   "metadata": {},
   "source": [
    "## 3. Handling Long Contexts\n",
    "\n",
    "To handle longer contexts, we'll implement a chunking strategy and use a sliding window approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6debb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create an optimized prompt for a Retrieval-Augmented Generation (RAG) model.\n",
    "def create_optimized_prompt_long(query, retrieved_docs, max_tokens=16385, max_docs=3, max_length_per_doc=1500, chunk_size=550, window_size=50):\n",
    "    \"\"\"\n",
    "    Create an optimized prompt with a limit on the total number of tokens, the number of documents,\n",
    "    and the length of each document. Implements a chunking strategy with a sliding window for longer contexts.\n",
    "    \"\"\"\n",
    "    # Initialize the prompt with a standard introduction and the user's query.\n",
    "    prompt = f\"\"\"Given the following query and relevant information from multiple sources, \n",
    "    provide a comprehensive and accurate answer. Synthesize the information from all sources,\n",
    "    and if there are any contradictions or gaps in the information, point them out.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Relevant Information:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a counter for the number of documents processed.\n",
    "    doc_count = 0\n",
    "    # Iterate over each document retrieved.\n",
    "    for i, doc in retrieved_docs.iterrows():\n",
    "        # Stop adding documents if the maximum number has been reached.\n",
    "        if doc_count >= max_docs:\n",
    "            break\n",
    "        # Extract the full text of the current document.\n",
    "        doc_text = doc['fullText']\n",
    "        # Split the document text into chunks using a sliding window approach.\n",
    "        chunks = [doc_text[i:i+chunk_size] for i in range(0, len(doc_text), chunk_size - window_size)]\n",
    "        # Iterate over each chunk, ensuring not to exceed the maximum document count or token limit.\n",
    "        for j, chunk in enumerate(chunks[:max_docs]):\n",
    "            if len(prompt) + len(chunk) > max_tokens:\n",
    "                break\n",
    "            # Add each chunk to the prompt, including a header indicating the source and part number.\n",
    "            prompt += f\"\\nSource {i+1}, Part {j+1} - {doc['title']}:\\n{chunk}\\n\"\n",
    "        # Increment the document counter.\n",
    "        doc_count += 1\n",
    "    \n",
    "    # Append a section for the synthesized answer to the prompt.\n",
    "    prompt += \"\\nSynthesized Answer:\"\n",
    "    \n",
    "    # If the prompt exceeds the maximum token limit, truncate it.\n",
    "    if len(prompt) > max_tokens:\n",
    "        prompt = prompt[:max_tokens]\n",
    "    \n",
    "    # Return the final prompt.\n",
    "    return prompt\n",
    "\n",
    "# Initialize a FAISSRetriever object with a DataFrame and a FAISS client.\n",
    "retriever = FAISSRetriever(df, client)\n",
    "# Define a query.\n",
    "query = \"How do different authors explore the concept of adaptation?\"\n",
    "# Retrieve documents relevant to the query using the retriever.\n",
    "retrieved = retriever.retrieve(query)  # Assume 'retriever' is a previously defined object with a 'retrieve' method.\n",
    "# Create an optimized prompt using the retrieved documents.\n",
    "optimized_prompt = create_optimized_prompt_long(query, retrieved)\n",
    "# Generate a response using the optimized prompt.\n",
    "response = get_completion(optimized_prompt)\n",
    "# Print the query and the generated response.\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Optimized RAG Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de74f6",
   "metadata": {},
   "source": [
    "## 4. Implementing Reranking Methods\n",
    "\n",
    "Let's implement a simple reranking method based on keyword matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to count the number of keyword matches in a document.\n",
    "def keyword_match_count(document, keywords):\n",
    "    # Convert the document text to lowercase and split it into words.\n",
    "    words = document.lower().split()\n",
    "    # Count the occurrences of each keyword in the document's words.\n",
    "    count = sum(word in keywords for word in words)\n",
    "    return count  # Return the total count of keyword matches.\n",
    "\n",
    "# Define a function to rerank documents based on keyword match count.\n",
    "def rerank_documents(retrieved_docs, keywords):\n",
    "    # Score each document by the number of keyword matches.\n",
    "    scored_documents = [(index, doc, keyword_match_count(doc['fullText'], keywords)) for index, doc in retrieved_docs.iterrows()]\n",
    "    # Sort the documents by their score in descending order.\n",
    "    scored_documents.sort(key=lambda x: x[2], reverse=True)\n",
    "    # Return the sorted documents, discarding the scores for simplicity.\n",
    "    return [doc for _, doc, _ in scored_documents]\n",
    "\n",
    "# Define a function to create an optimized prompt for a RAG model, incorporating document reranking.\n",
    "def create_optimized_prompt_long(query, retrieved_docs, keywords, max_tokens=16385, max_docs=3, max_length_per_doc=1500, chunk_size=550, window_size=50):\n",
    "    \"\"\"\n",
    "    This function reranks documents based on keyword matching before creating the prompt.\n",
    "    It ensures the prompt is constructed from the most relevant documents.\n",
    "    \"\"\"\n",
    "    # Start constructing the prompt with the query and an introduction.\n",
    "    prompt = f\"\"\"Given the following query and relevant information from multiple sources, \n",
    "    provide a comprehensive and accurate answer. Synthesize the information from all sources,\n",
    "    and if there are any contradictions or gaps in the information, point them out.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Relevant Information:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rerank the documents based on keyword matching.\n",
    "    reranked_docs = rerank_documents(retrieved_docs, keywords)\n",
    "    \n",
    "    # Initialize a counter for the number of documents added to the prompt.\n",
    "    doc_count = 0\n",
    "    # Iterate over the reranked documents to add their content to the prompt.\n",
    "    for i, doc in enumerate(reranked_docs):\n",
    "        if doc_count >= max_docs:\n",
    "            break  # Stop if the maximum number of documents has been reached.\n",
    "        doc_text = doc['fullText']\n",
    "        # Break the document text into chunks to fit within the prompt size limits.\n",
    "        chunks = [doc_text[i:i+chunk_size] for i in range(0, len(doc_text), chunk_size - window_size)]\n",
    "        for j, chunk in enumerate(chunks[:max_docs]):\n",
    "            if len(prompt) + len(chunk) > max_tokens:\n",
    "                break  # Stop adding chunks if the prompt would exceed the token limit.\n",
    "            # Add the chunk to the prompt, including a header with the source number and part.\n",
    "            prompt += f\"\\nSource {i+1}, Part {j+1} - {doc['title']}:\\n{chunk}\\n\"\n",
    "        doc_count += 1  # Increment the document counter.\n",
    "    \n",
    "    # Finalize the prompt with a section for the synthesized answer.\n",
    "    prompt += \"\\nSynthesized Answer:\"\n",
    "    \n",
    "    # Trim the prompt if it exceeds the maximum token limit.\n",
    "    if len(prompt) > max_tokens:\n",
    "        prompt = prompt[:max_tokens]\n",
    "    \n",
    "    return prompt  # Return the constructed prompt.\n",
    "\n",
    "# Example usage of the functions to generate an optimized RAG response.\n",
    "keywords = [\"adaptation\", \"authors\", \"concept\"]  # Define relevant keywords for reranking.\n",
    "retrieved = retriever.retrieve(query)  # Retrieve documents relevant to the query.\n",
    "optimized_prompt = create_optimized_prompt_long(query, retrieved, keywords)  # Adjusted to include keywords for reranking.\n",
    "response = get_completion(optimized_prompt)  # Generate a response using the optimized prompt.\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Optimized RAG Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1022a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PorterStemmer class for stemming words\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Define a function to count the overlap of stemmed keywords with stemmed document tokens\n",
    "def enhanced_keyword_match_count(document, keywords):\n",
    "    stemmer = PorterStemmer()  # Initialize the stemmer\n",
    "    # Tokenize and stem the document text\n",
    "    doc_tokens = word_tokenize(document.lower())\n",
    "    doc_stems = [stemmer.stem(token) for token in doc_tokens]\n",
    "    # Stem the keywords for comparison\n",
    "    keyword_stems = [stemmer.stem(keyword.lower()) for keyword in keywords]\n",
    "    # Count the occurrences of each stemmed keyword in the document's stemmed tokens\n",
    "    count = sum(stem in doc_stems for stem in keyword_stems)\n",
    "    return count  # Return the total count of keyword occurrences\n",
    "\n",
    "# Function to rerank documents based on the count of keyword matches\n",
    "def rerank_documents(retrieved_docs, keywords):\n",
    "    # Ensure the input is a DataFrame for easier manipulation\n",
    "    if not isinstance(retrieved_docs, pd.DataFrame):\n",
    "        retrieved_docs = pd.DataFrame(retrieved_docs)\n",
    "    # Score each document by the number of keyword matches\n",
    "    scored_documents = [(index, doc, enhanced_keyword_match_count(doc['fullText'], keywords)) for index, doc in retrieved_docs.iterrows()]\n",
    "    # Sort the documents by their score in descending order\n",
    "    scored_documents.sort(key=lambda x: x[2], reverse=True)\n",
    "    # Return the sorted documents, discarding the scores for simplicity\n",
    "    return [doc for _, doc, _ in scored_documents]\n",
    "\n",
    "# Function to create an optimized prompt for a RAG model, incorporating document reranking\n",
    "def create_optimized_prompt_long(query, retrieved_docs, keywords, max_tokens=16385, max_docs=3, max_length_per_doc=1500, chunk_size=550, window_size=50):\n",
    "    \"\"\"\n",
    "    This function reranks documents based on keyword matching before creating the prompt.\n",
    "    It ensures the prompt is constructed from the most relevant documents.\n",
    "    \"\"\"\n",
    "    # Start constructing the prompt with the query and an introduction\n",
    "    prompt = f\"\"\"Given the following query and relevant information from multiple sources, \n",
    "    provide a comprehensive and accurate answer. Synthesize the information from all sources,\n",
    "    and if there are any contradictions or gaps in the information, point them out.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Relevant Information:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rerank the documents based on keyword matching\n",
    "    reranked_docs = rerank_documents(retrieved_docs, keywords)\n",
    "    \n",
    "    # Initialize a counter for the number of documents added to the prompt\n",
    "    doc_count = 0\n",
    "    # Iterate over the reranked documents to add their content to the prompt\n",
    "    for i, doc in enumerate(reranked_docs):\n",
    "        if doc_count >= max_docs:\n",
    "            break  # Stop if the maximum number of documents has been reached\n",
    "        doc_text = doc['fullText']\n",
    "        # Break the document text into chunks to fit within the prompt size limits\n",
    "        chunks = [doc_text[i:i+chunk_size] for i in range(0, len(doc_text), chunk_size - window_size)]\n",
    "        for j, chunk in enumerate(chunks[:max_docs]):\n",
    "            if len(prompt) + len(chunk) > max_tokens:\n",
    "                break  # Stop adding chunks if the prompt would exceed the token limit\n",
    "            # Add the chunk to the prompt, including a header with the source number and part\n",
    "            prompt += f\"\\nSource {i+1}, Part {j+1} - {doc['title']}:\\n{chunk}\\n\"\n",
    "        doc_count += 1  # Increment the document counter\n",
    "    \n",
    "    # Finalize the prompt with a section for the synthesized answer\n",
    "    prompt += \"\\nSynthesized Answer:\"\n",
    "    \n",
    "    # Trim the prompt if it exceeds the maximum token limit\n",
    "    if len(prompt) > max_tokens:\n",
    "        prompt = prompt[:max_tokens]\n",
    "    \n",
    "    return prompt  # Return the constructed prompt\n",
    "\n",
    "# Example usage of the functions to generate an optimized RAG response\n",
    "keywords = [\"adaptation\", \"authors\", \"concept\"]  # Define relevant keywords for reranking\n",
    "retrieved = retriever.retrieve(query)  # Retrieve documents relevant to the query.\n",
    "optimized_prompt = create_optimized_prompt_long(query, retrieved, keywords)  # Adjusted to include keywords for reranking\n",
    "response = get_completion(optimized_prompt)  # Generate a response using the optimized prompt.\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Optimized RAG Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5565664f",
   "metadata": {},
   "source": [
    "## 5. Building an Optimized RAG Pipeline\n",
    "\n",
    "Now, let's put everything together into an optimized RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d39e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of keyword matches in a document\n",
    "def enhanced_keyword_match_count(document, keywords):\n",
    "    # Initialize the Porter Stemmer for stemming words\n",
    "    stemmer = PorterStemmer()\n",
    "    # Tokenize and stem the document text\n",
    "    doc_tokens = word_tokenize(document.lower())\n",
    "    doc_stems = set(stemmer.stem(token) for token in doc_tokens)\n",
    "    # Stem the keywords for matching\n",
    "    keyword_stems = set(stemmer.stem(keyword.lower()) for keyword in keywords)\n",
    "    # Count the number of keyword stems present in the document stems\n",
    "    count = sum(stem in doc_stems for stem in keyword_stems)\n",
    "    return count\n",
    "\n",
    "# Function to rerank documents based on the number of keyword matches\n",
    "def rerank_documents(retrieved_docs, keywords):\n",
    "    # Convert retrieved_docs to a DataFrame if it's not already one\n",
    "    if not isinstance(retrieved_docs, pd.DataFrame):\n",
    "        retrieved_docs = pd.DataFrame(retrieved_docs)\n",
    "    # Score each document by the number of keyword matches\n",
    "    scored_documents = [(index, doc, enhanced_keyword_match_count(doc['fullText'], keywords)) for index, doc in retrieved_docs.iterrows()]\n",
    "    # Sort the documents by their score in descending order\n",
    "    scored_documents.sort(key=lambda x: x[2], reverse=True)\n",
    "    # Return the sorted documents, discarding the scores\n",
    "    return [doc for _, doc, _ in scored_documents]\n",
    "\n",
    "# Function to create an optimized prompt for a RAG model\n",
    "def create_optimized_prompt_long(query, retrieved_docs, keywords, max_tokens=16385, max_docs=3, max_length_per_doc=1500, chunk_size=550, window_size=50):\n",
    "    # Initial prompt setup with the user's query\n",
    "    prompt = f\"\"\"Given the following query and relevant information from multiple sources, \n",
    "    provide a comprehensive and accurate answer. Synthesize the information from all sources,\n",
    "    and if there are any contradictions or gaps in the information, point them out.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Relevant Information:\n",
    "    \"\"\"\n",
    "    # Rerank documents based on keyword matching\n",
    "    reranked_docs = rerank_documents(retrieved_docs, keywords)\n",
    "    doc_count = 0\n",
    "    # Iterate over the reranked documents to add them to the prompt\n",
    "    for i, doc in enumerate(reranked_docs):\n",
    "        if doc_count >= max_docs:\n",
    "            break\n",
    "        doc_text = doc['fullText']\n",
    "        # Break the document text into chunks for inclusion in the prompt\n",
    "        chunks = [doc_text[i:i+chunk_size] for i in range(0, len(doc_text), chunk_size - window_size)]\n",
    "        for j, chunk in enumerate(chunks[:max_docs]):\n",
    "            # Ensure the prompt does not exceed the maximum token limit\n",
    "            if len(prompt) + len(chunk) > max_tokens:\n",
    "                break\n",
    "            # Add the chunk to the prompt\n",
    "            prompt += f\"\\nSource {i+1}, Part {j+1} - {doc['title']}:\\n{chunk}\\n\"\n",
    "        doc_count += 1\n",
    "    # Finalize the prompt with a section for the synthesized answer\n",
    "    prompt += \"\\nSynthesized Answer:\"\n",
    "    # Trim the prompt if it exceeds the maximum token limit\n",
    "    if len(prompt) > max_tokens:\n",
    "        prompt = prompt[:max_tokens]\n",
    "    return prompt\n",
    "\n",
    "# Example usage of the functions to generate an optimized RAG response\n",
    "query = \"How do literary scholars understand adaptation theory?\"\n",
    "keywords = [\"adaptation\", \"authors\", \"concept\"]\n",
    "# Assume a retriever function is defined elsewhere to get relevant documents\n",
    "retrieved = retriever.retrieve(query)\n",
    "# Generate an optimized prompt based on the retrieved documents and keywords\n",
    "optimized_prompt = create_optimized_prompt_long(query, retrieved, keywords)\n",
    "# Assume a get_completion function is defined elsewhere to generate a response\n",
    "response = get_completion(optimized_prompt)\n",
    "# Print the query and the optimized RAG response\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Optimized RAG Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5853194",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Implement a cross-encoder reranking method using a pre-trained model from the `sentence-transformers` library.\n",
    "\n",
    "2. Develop a method to dynamically adjust the number of retrieved documents based on the complexity of the query.\n",
    "\n",
    "3. Implement a simple caching mechanism to store and reuse embeddings and retrieved results for frequently asked questions.\n",
    "\n",
    "4. Create a method to generate follow-up questions based on the initial RAG response, encouraging a more interactive and in-depth exploration of the topic.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this final lesson of our LLMs with RAG workshop, we've explored advanced techniques for optimizing Retrieval Augmented Generation systems. We've implemented efficient retrieval using FAISS, developed strategies for handling long contexts, created optimized prompts, and built a reranking method. Finally, we combined these techniques into a comprehensive RAG pipeline.\n",
    "\n",
    "These optimization techniques can significantly enhance the performance of RAG systems, making them more accurate, efficient, and capable of handling complex queries and large knowledge bases.\n",
    "\n",
    "As you continue to work with RAG systems, remember that ongoing experimentation and refinement are key to achieving the best results for your specific use case.\n",
    "\n",
    "# References\n",
    "\n",
    "1. Karpukhin, V., et al. (2020). [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906). arXiv preprint arXiv:2004.04906.\n",
    "2. Khattab, O., & Zaharia, M. (2020). [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/abs/2004.12832). arXiv preprint arXiv:2004.12832.\n",
    "3. Gao, L., et al. (2021). [Making Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/abs/2012.15723). arXiv preprint arXiv:2012.15723.\n",
    "4. Johnson, J., Douze, M., & Jégou, H. (2019). [Billion-scale similarity search with GPUs](https://arxiv.org/abs/1702.08734). IEEE Transactions on Big Data.\n",
    "\n",
    "___\n",
    "This concludes our LLMs with RAG workshop series. I hope you found these lessons informative and practical for your research and applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
