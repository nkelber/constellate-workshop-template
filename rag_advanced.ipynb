{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Grant Glass](https://glassgrant.com) for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email grantg@unc.edu.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# `Large Language Models and Embeddings for Retrieval Augmented Generation: Day 3` `7/19/24`\n",
    "\n",
    "This is lesson `3` of 3 in the educational series on `Large Language Models (LLMs) and Retrieval Augmented Generation (RAG)`. This notebook focuses on advanced techniques for optimizing RAG systems.\n",
    "\n",
    "**Skills:** \n",
    "* Data analysis\n",
    "* Machine learning\n",
    "* Text analysis\n",
    "* Language models\n",
    "* Vector embeddings\n",
    "* Retrieval Augmented Generation\n",
    "* Performance optimization\n",
    "\n",
    "**Audience:** `Learners`\n",
    "\n",
    "**Use case:** `Tutorial`\n",
    "\n",
    "This tutorial guides users through advanced techniques for optimizing Retrieval Augmented Generation systems.\n",
    "\n",
    "\n",
    "\n",
    "**Difficulty:** `Advanced`\n",
    "\n",
    "Advanced assumes users are very familiar with Python and have been programming for years, but they may not be familiar with the specific optimization techniques for RAG systems.\n",
    "\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python programming (including object-oriented programming)\n",
    "* Understanding of LLMs and embeddings (covered in Days 1 and 2)\n",
    "* Basic knowledge of RAG systems (covered in Day 2)\n",
    "\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "* Experience with natural language processing (NLP)\n",
    "* Familiarity with information retrieval concepts\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "1. Implement advanced retrieval techniques for RAG systems\n",
    "2. Optimize prompt engineering for improved RAG performance\n",
    "3. Develop strategies for handling long contexts in RAG\n",
    "4. Implement and evaluate different reranking methods\n",
    "5. Create a more sophisticated RAG pipeline integrating multiple optimization techniques\n",
    "\n",
    "\n",
    "**Research Pipeline:**\n",
    "1. Introduction to LLMs and their applications (Day 1)\n",
    "2. Exploring embeddings and introduction to RAG (Day 2)\n",
    "3. **Optimizing RAG systems for enhanced performance**\n",
    "4. Applying optimized RAG in research contexts\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "\n",
    "* [OpenAI](https://github.com/openai/openai-python) for generating embeddings and interacting with GPT models\n",
    "* [Pandas](https://pandas.pydata.org/) for data manipulation\n",
    "* [NumPy](https://numpy.org/) for numerical operations\n",
    "* [Scikit-learn](https://scikit-learn.org/) for similarity calculations and evaluation metrics\n",
    "* [FAISS](https://github.com/facebookresearch/faiss) for efficient similarity search\n",
    "* [NLTK](https://www.nltk.org/) for text preprocessing\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install Libraries ###\n",
    "!pip install openai pandas numpy scikit-learn faiss-cpu nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "# Import the OpenAI library to interact with the OpenAI API, useful for tasks like text generation or semantic search.\n",
    "import openai\n",
    "\n",
    "# Import pandas, a powerful data manipulation and analysis library for Python.\n",
    "import pandas as pd\n",
    "\n",
    "# Import numpy, a library for numerical operations on large, multi-dimensional arrays and matrices.\n",
    "import numpy as np\n",
    "\n",
    "# Import cosine_similarity from sklearn, a method for calculating the cosine similarity between vectors, useful in various machine learning tasks.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Import precision_score, recall_score, f1_score from sklearn for evaluating the accuracy of a classification.\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Import faiss, a library for efficient similarity search and clustering of dense vectors.\n",
    "import faiss\n",
    "\n",
    "# Import nltk, a toolkit for natural language processing (NLP) tasks.\n",
    "import nltk\n",
    "\n",
    "# From nltk, import word_tokenize for splitting strings into words and stopwords for filtering out common words.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import os, a module for interacting with the operating system, useful for file paths, environment variables, etc.\n",
    "import os\n",
    "\n",
    "# The line intended to import the faiss library is misspelled. It should be \"import faiss\".\n",
    "# Faiss is a library for efficient similarity search and clustering of dense vectors.\n",
    "import faiss\n",
    "\n",
    "# Import the ast library, which is used for processing trees of the Python abstract syntax grammar.\n",
    "# The ast module helps in introspecting and analyzing Python code.\n",
    "# Add this import to handle string to list conversion\n",
    "import ast  \n",
    "\n",
    "# Download the 'punkt' tokenizer models. This is necessary for tokenizing words in text.\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download the list of stopwords to filter out common words that are usually irrelevant in NLP tasks.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedd148",
   "metadata": {},
   "source": [
    "# Required Data\n",
    "\n",
    "We'll continue using the dataset from Day 2.\n",
    "\n",
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9da404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('day2_dataset.csv')  # Assuming we saved the DataFrame from Day 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this final lesson of our LLMs with RAG workshop, we'll focus on advanced techniques for optimizing Retrieval Augmented Generation (RAG) systems. We'll explore methods to enhance retrieval accuracy, improve prompt engineering, handle long contexts, and implement reranking strategies.\n",
    "\n",
    "Key topics we'll cover:\n",
    "1. Advanced retrieval techniques\n",
    "2. Optimizing prompt engineering\n",
    "3. Handling long contexts\n",
    "4. Implementing reranking methods\n",
    "5. Building an optimized RAG pipeline\n",
    "\n",
    "Let's begin by setting up our OpenAI API access and defining some utility functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996cb30a",
   "metadata": {},
   "source": [
    "## Configure the OpenAI client\n",
    "\n",
    "To setup the client for our use, we need to create an API key to use with our request. Skip these steps if you already have an API key for usage.\n",
    "\n",
    "You can get an API key by following these steps:\n",
    "\n",
    "1. [Create a new project](https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects)\n",
    "2. [Generate an API key in your project](https://platform.openai.com/api-keys)\n",
    "3. (RECOMMENDED, BUT NOT REQUIRED) [Setup your API key for all projects as an env var](https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d06be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the API key and client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as an env var>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the embedding of a given text.\n",
    "# This function takes a text string and an optional model name as input.\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    # Replace newline characters with spaces in the text to ensure it's in a single line.\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Use the OpenAI API client to create an embedding for the text using the specified model.\n",
    "    # The function returns the embedding of the first (and only) input text.\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "# Define a function to get a completion for a given prompt using GPT.\n",
    "# This function takes a prompt string and an optional model name as input.\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    # Structure the prompt into a format suitable for the OpenAI API, specifying the role as \"user\".\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # Use the OpenAI API client to create a chat completion using the specified model.\n",
    "    # The temperature is set to 0 for deterministic output, meaning no randomness in the response.\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    # Return the content of the first (and only) message in the response.\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Print a message indicating that the OpenAI API is ready for use.\n",
    "print(\"OpenAI API is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0042720",
   "metadata": {},
   "source": [
    "# Lesson\n",
    "\n",
    "## 1. Advanced Retrieval Techniques\n",
    "\n",
    "Let's implement a more sophisticated retrieval system using FAISS for efficient similarity search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a24518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for retrieving documents using FAISS (Facebook AI Similarity Search)\n",
    "class FAISSRetriever:\n",
    "    # Initialize the retriever with a DataFrame containing the documents and their embeddings\n",
    "    def __init__(self, df):\n",
    "        self.df = df  # Store the DataFrame\n",
    "        self.index = None  # Initialize the FAISS index as None\n",
    "        # Check if the DataFrame is not empty, contains an 'embedding' column, and all embeddings are not null\n",
    "        if not df.empty and 'embedding' in df.columns and not df['embedding'].isnull().all():\n",
    "            self.build_index()  # Build the FAISS index\n",
    "        else:\n",
    "            # If conditions are not met, print a warning and do not build the index\n",
    "            print(\"DataFrame is empty or does not contain valid embeddings. Index will not be built.\")\n",
    "\n",
    "    # Method to build the FAISS index\n",
    "    def build_index(self):\n",
    "        try:\n",
    "            # Check if embeddings are stored as strings; if so, convert them to lists\n",
    "            if isinstance(self.df['embedding'].iloc[0], str):\n",
    "                # Convert string embeddings to lists using ast.literal_eval and stack them vertically\n",
    "                embeddings = np.vstack([ast.literal_eval(emb) for emb in self.df['embedding'].values])\n",
    "            else:\n",
    "                # If embeddings are not strings, stack them vertically as they are\n",
    "                embeddings = np.vstack(self.df['embedding'].values)\n",
    "        except ValueError as e:\n",
    "            # Catch and print any ValueError that occurs during stacking\n",
    "            print(f\"Error stacking embeddings: {e}\")\n",
    "            return\n",
    "\n",
    "        # Check if the embeddings are a 2D array as expected\n",
    "        if embeddings.ndim != 2:\n",
    "            print(f\"Expected embeddings to be a 2D array, got {embeddings.ndim}D instead.\")\n",
    "            return\n",
    "\n",
    "        # Create a FAISS index for L2 distance and add the embeddings\n",
    "        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "\n",
    "    # Method to retrieve documents similar to a given query\n",
    "    def retrieve(self, query, k=3):\n",
    "        # Check if the index is built\n",
    "        if self.index is None:\n",
    "            print(\"Index is not built. Cannot retrieve documents.\")\n",
    "            return None\n",
    "        # Convert the query to an embedding and search the index for the k nearest neighbors\n",
    "        query_embedding = np.array([get_embedding(query)]).astype('float32')\n",
    "        _, indices = self.index.search(query_embedding, k)\n",
    "        # Return the DataFrame rows corresponding to the indices of the retrieved documents\n",
    "        return self.df.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce790d",
   "metadata": {},
   "source": [
    "## 2. Optimizing Prompt Engineering\n",
    "\n",
    "Let's create a more sophisticated prompt template that includes multiple retrieved documents and encourages the model to synthesize information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_prompt(query, retrieved_docs, max_tokens=16385, max_docs=3, max_length_per_doc=500):\n",
    "    \"\"\"\n",
    "    Create an optimized prompt with a limit on the total number of tokens, the number of documents,\n",
    "    and the length of each document.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Given the following query and relevant information from multiple sources, \n",
    "    provide a comprehensive and accurate answer. Synthesize the information from all sources,\n",
    "    and if there are any contradictions or gaps in the information, point them out.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Relevant Information:\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_count = 0\n",
    "    for i, doc in retrieved_docs.iterrows():\n",
    "        if doc_count >= max_docs:\n",
    "            break\n",
    "        doc_text = doc['fullText'][:max_length_per_doc]  # Truncate document\n",
    "        prompt += f\"\\nSource {i+1} - {doc['title']}:\\n{doc_text}\\n\"\n",
    "        doc_count += 1\n",
    "    \n",
    "    prompt += \"\\nSynthesized Answer:\"\n",
    "    \n",
    "    # Check if the prompt exceeds the maximum token limit\n",
    "    if len(prompt) > max_tokens:\n",
    "        prompt = prompt[:max_tokens]  # Truncate the prompt to fit the token limit\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example usage\n",
    "query = \"How do different authors explore the theme of societal control?\"\n",
    "retrieved = retriever.retrieve(query)  # Corrected this line\n",
    "optimized_prompt = create_optimized_prompt(query, retrieved)\n",
    "response = get_completion(optimized_prompt)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Optimized RAG Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cce476",
   "metadata": {},
   "source": [
    "## 3. Handling Long Contexts\n",
    "\n",
    "To handle longer contexts, we'll implement a chunking strategy and use a sliding window approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cfd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create an optimized prompt for text generation tasks.\n",
    "# This function limits the total number of tokens, the number of documents, and the length of each document.\n",
    "def create_optimized_prompt(query, retrieved_docs, max_tokens=16385, max_docs=3, max_length_per_doc=500):\n",
    "    \"\"\"\n",
    "    Create an optimized prompt with a limit on the total number of tokens, the number of documents,\n",
    "    and the length of each document.\n",
    "    \"\"\"\n",
    "    # Start building the prompt with an introduction and the query.\n",
    "    prompt = f\"\"\"Given the following query and relevant information from multiple sources, \n",
    "    provide a comprehensive and accurate answer. Synthesize the information from all sources,\n",
    "    and if there are any contradictions or gaps in the information, point them out.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Relevant Information:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a counter for the number of documents added to the prompt.\n",
    "    doc_count = 0\n",
    "    # Iterate over the retrieved documents.\n",
    "    for i, doc in retrieved_docs.iterrows():\n",
    "        # Stop adding documents if the maximum number of documents has been reached.\n",
    "        if doc_count >= max_docs:\n",
    "            break\n",
    "        # Truncate the document text to the maximum length per document.\n",
    "        doc_text = doc['fullText'][:max_length_per_doc]\n",
    "        # Add the document title and truncated text to the prompt.\n",
    "        prompt += f\"\\nSource {i+1} - {doc['title']}:\\n{doc_text}\\n\"\n",
    "        # Increment the document counter.\n",
    "        doc_count += 1\n",
    "    \n",
    "    # Append a section for the synthesized answer to the prompt.\n",
    "    prompt += \"\\nSynthesized Answer:\"\n",
    "    \n",
    "    # Check if the prompt exceeds the maximum token limit.\n",
    "    if len(prompt) > max_tokens:\n",
    "        # Truncate the prompt to fit the token limit.\n",
    "        prompt = prompt[:max_tokens]\n",
    "    \n",
    "    # Return the constructed prompt.\n",
    "    return prompt\n",
    "\n",
    "# Example usage of the function.\n",
    "# Define a query.\n",
    "query = \"How do different authors explore the theme of societal control?\"\n",
    "# Retrieve documents relevant to the query.\n",
    "retrieved = retriever.retrieve(query)  # Assume 'retriever' is a previously defined object with a 'retrieve' method.\n",
    "# Create an optimized prompt using the retrieved documents.\n",
    "optimized_prompt = create_optimized_prompt(query, retrieved)\n",
    "# Generate a response using the optimized prompt.\n",
    "response = get_completion(optimized_prompt)\n",
    "# Print the query and the generated response.\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Optimized RAG Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de74f6",
   "metadata": {},
   "source": [
    "## 4. Implementing Reranking Methods\n",
    "\n",
    "Let's implement a simple reranking method based on keyword matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce14aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to rerank retrieved documents based on keyword overlap with the query.\n",
    "def keyword_reranker(query, retrieved_docs, top_k=2):\n",
    "    # Tokenize the query, convert to lowercase, remove stopwords, and create a set of unique keywords.\n",
    "    query_keywords = set(word_tokenize(query.lower())) - set(stopwords.words('english'))\n",
    "    \n",
    "    # Define a function to calculate the keyword score for a given text.\n",
    "    # The score is the number of query keywords present in the text.\n",
    "    def keyword_score(text):\n",
    "        # Tokenize the text, convert to lowercase, remove stopwords, and create a set of unique keywords.\n",
    "        text_keywords = set(word_tokenize(text.lower())) - set(stopwords.words('english'))\n",
    "        # Return the count of query keywords found in the text's keywords.\n",
    "        return len(query_keywords.intersection(text_keywords))\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning when modifying it.\n",
    "    retrieved_docs_copy = retrieved_docs.copy()\n",
    "    # Apply the keyword_score function to each document's fullText and store the results in a new column.\n",
    "    retrieved_docs_copy['keyword_score'] = retrieved_docs_copy['fullText'].apply(keyword_score)\n",
    "    # Sort the documents by their keyword score in descending order and return the top_k documents.\n",
    "    return retrieved_docs_copy.sort_values('keyword_score', ascending=False).head(top_k)\n",
    "\n",
    "# Example usage of the function.\n",
    "# Define a query.\n",
    "query = \"A story about slavery and its impact on society\"\n",
    "# Retrieve documents relevant to the query, assuming 'retriever' is a previously defined object.\n",
    "retrieved = retriever.retrieve(query, k=5)\n",
    "# Rerank the retrieved documents based on keyword overlap with the query.\n",
    "reranked = keyword_reranker(query, retrieved)\n",
    "# Print the top 2 reranked results, including their titles, full texts, and keyword scores.\n",
    "print(f\"Top 2 reranked results for '{query}':\")\n",
    "print(reranked[['title', 'fullText', 'keyword_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5565664f",
   "metadata": {},
   "source": [
    "## 5. Building an Optimized RAG Pipeline\n",
    "\n",
    "Now, let's put everything together into an optimized RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b56e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class named OptimizedRAG for retrieving and generating answers using a Retriever-Generator approach.\n",
    "class OptimizedRAG:\n",
    "    # Initialize the OptimizedRAG object with a DataFrame containing documents and their embeddings.\n",
    "    def __init__(self, df):\n",
    "        # Create a FAISSRetriever instance using the provided DataFrame for document retrieval.\n",
    "        self.retriever = FAISSRetriever(df)\n",
    "    \n",
    "    # Define a method to get an answer for a given query.\n",
    "    def get_answer(self, query):\n",
    "        # Use the retriever to fetch initial candidate documents based on the query, limiting to top 5.\n",
    "        retrieved = self.retriever.retrieve(query, k=5)\n",
    "        \n",
    "        # Rerank the retrieved documents using keyword overlap with the query, selecting the top 3.\n",
    "        reranked = keyword_reranker(query, retrieved, top_k=3)\n",
    "        \n",
    "        # Start building the prompt for the language model, including an instruction and the query.\n",
    "        prompt = f\"\"\"Given the following query and relevant information from multiple sources, \n",
    "        provide a comprehensive and accurate answer. Synthesize the information from all sources,\n",
    "        and if there are any contradictions or gaps in the information, point them out.\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Relevant Information:\n",
    "        \"\"\"\n",
    "        \n",
    "        # For each reranked document, append its title and a truncated version of its text to the prompt.\n",
    "        for _, doc in reranked.iterrows():\n",
    "            prompt += f\"\\nSource: {doc['title']}:\\n{doc['fullText'][:500]}...\\n\"\n",
    "        \n",
    "        # Append a section to the prompt where the synthesized answer will be placed.\n",
    "        prompt += \"\\nSynthesized Answer:\"\n",
    "        \n",
    "        # Use the get_completion function to generate a response from the language model based on the prompt.\n",
    "        response = get_completion(prompt)\n",
    "        \n",
    "        # Return the generated response.\n",
    "        return response\n",
    "\n",
    "# Create an instance of the OptimizedRAG system with a DataFrame 'df' containing documents and their embeddings.\n",
    "rag_system = OptimizedRAG(df)\n",
    "\n",
    "# Example usage of the OptimizedRAG system.\n",
    "# Define a query about Frederick Douglass' views on slavery and freedom.\n",
    "query = \"How did Frederick Douglass' experiences shape his views on slavery and freedom?\"\n",
    "# Get an answer for the query using the OptimizedRAG system.\n",
    "answer = rag_system.get_answer(query)\n",
    "# Print the query and the optimized answer generated by the system.\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Optimized RAG Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5853194",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Implement a cross-encoder reranking method using a pre-trained model from the `sentence-transformers` library.\n",
    "\n",
    "2. Develop a method to dynamically adjust the number of retrieved documents based on the complexity of the query.\n",
    "\n",
    "3. Implement a simple caching mechanism to store and reuse embeddings and retrieved results for frequently asked questions.\n",
    "\n",
    "4. Create a method to generate follow-up questions based on the initial RAG response, encouraging a more interactive and in-depth exploration of the topic.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this final lesson of our LLMs with RAG workshop, we've explored advanced techniques for optimizing Retrieval Augmented Generation systems. We've implemented efficient retrieval using FAISS, developed strategies for handling long contexts, created optimized prompts, and built a reranking method. Finally, we combined these techniques into a comprehensive RAG pipeline.\n",
    "\n",
    "These optimization techniques can significantly enhance the performance of RAG systems, making them more accurate, efficient, and capable of handling complex queries and large knowledge bases.\n",
    "\n",
    "As you continue to work with RAG systems, remember that ongoing experimentation and refinement are key to achieving the best results for your specific use case.\n",
    "\n",
    "# References\n",
    "\n",
    "1. Karpukhin, V., et al. (2020). [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906). arXiv preprint arXiv:2004.04906.\n",
    "2. Khattab, O., & Zaharia, M. (2020). [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/abs/2004.12832). arXiv preprint arXiv:2004.12832.\n",
    "3. Gao, L., et al. (2021). [Making Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/abs/2012.15723). arXiv preprint arXiv:2012.15723.\n",
    "4. Johnson, J., Douze, M., & JÃ©gou, H. (2019). [Billion-scale similarity search with GPUs](https://arxiv.org/abs/1702.08734). IEEE Transactions on Big Data.\n",
    "\n",
    "___\n",
    "This concludes our LLMs with RAG workshop series. I hope you found these lessons informative and practical for your research and applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
