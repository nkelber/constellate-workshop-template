{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Grant Glass](https://glassgrant.com) for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email grantg@unc.edu.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Large Language Models and Embeddings for Retrieval Augmented Generation: Day 1 7/15/24\n",
    "\n",
    "This is lesson `1` of 3 in the educational series on `Large Language Models (LLMs) and Retrieval Augmented Generation (RAG)`. This notebook is intended to introduce the concepts of LLMs and provide hands-on experience with analyzing their capabilities and limitations.\n",
    "\n",
    "**Skills:** \n",
    "* Data analysis\n",
    "* Machine learning\n",
    "* Text analysis\n",
    "* Language models\n",
    "\n",
    "**Audience:** `Learners`\n",
    "\n",
    "**Use case:** `Tutorial`\n",
    "\n",
    "This tutorial guides users through the process of understanding and analyzing Large Language Models, providing step-by-step instructions and explanations.\n",
    "\n",
    "\n",
    "**Difficulty:** `Intermediate`\n",
    "\n",
    "Intermediate assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.\n",
    "\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "* Basic understanding of machine learning concepts\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "* Familiarity with natural language processing (NLP) concepts\n",
    "* Experience with data manipulation libraries like Pandas\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "1. Describe the fundamental concepts and architecture of Large Language Models\n",
    "2. Implement basic prompts and analyze LLM responses\n",
    "3. Evaluate LLM performance on various tasks\n",
    "4. Discuss the strengths and limitations of current LLM technology\n",
    "\n",
    "**Research Pipeline:**\n",
    "1. Introduction to LLMs and their applications\n",
    "2. **Hands-on analysis of LLM capabilities**\n",
    "3. Exploring embeddings and RAG (Day 2)\n",
    "4. Optimizing RAG systems (Day 3)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "* [OpenAI](https://github.com/openai/openai-python) for interacting with GPT models\n",
    "* [Pandas](https://pandas.pydata.org/) for data manipulation and analysis\n",
    "* [Matplotlib](https://matplotlib.org/) for data visualization\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install Libraries ###\n",
    "!pip install openai pandas matplotlib tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "# Import the openai library for accessing OpenAI's API functionalities\n",
    "import openai\n",
    "# Import the OpenAI class from the openai library for more specific API interactions\n",
    "from openai import OpenAI\n",
    "# Import the os library to interact with the operating system, like reading or writing files\n",
    "import os\n",
    "# Import pandas, a powerful data manipulation and analysis library, as 'pd'\n",
    "import pandas as pd\n",
    "# Import matplotlib's pyplot to create static, interactive, and animated visualizations in Python, as 'plt'\n",
    "import matplotlib.pyplot as plt\n",
    "# Import the constellate library for working with datasets and analytics\n",
    "import constellate\n",
    "# Import the dataset_reader function from the constellate library to read and process datasets\n",
    "from constellate import dataset_reader\n",
    "# Import time\n",
    "import time\n",
    "# Import tiktoken which helps us count tokens\n",
    "import tiktoken\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedd148",
   "metadata": {},
   "source": [
    "## Import your dataset\n",
    "\n",
    "The next code cell tries to import your dataset using the following method:\n",
    "\n",
    "* Download a full dataset that has been requested\n",
    "\n",
    "\n",
    "If you are using a [dataset ID](https://constellate.org/docs/key-terms/#dataset-ID), replace the default dataset ID in the next code cell.\n",
    "\n",
    "If you don't have a dataset ID, you can:\n",
    "* Use the sample dataset ID already in the code cell\n",
    "* [Create a new dataset](https://constellate.org/builder)\n",
    "* [Use a dataset ID from other pre-built sample datasets](https://constellate.org/dataset/dashboard)\n",
    "\n",
    "The Constellate client will download datasets automatically using either the `.download()` or `.get_dataset()` method.\n",
    "* Full datasets are downloaded using the `.download()` method. They must be requested first in the builder environment. See the [Constellate client documentation](https://constellate.org/docs/constellate-client).\n",
    "\n",
    "* Sampled datasets (1500 items) are downloaded using the `.get_dataset()` method. They are built automatically when a dataset is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9da404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assign a specific dataset ID to the variable `dataset_id` you will get this in the Constellate dataset builder\n",
    "dataset_id = \"716ce175-4d27-6e6e-2f3d-defaa1f0d81d\"\n",
    "\n",
    "# Use the constellate library to download the dataset specified by `dataset_id` in JSON Lines format\n",
    "dataset_file = constellate.download(dataset_id, 'jsonl')\n",
    "\n",
    "# Initialize empty lists to hold various pieces of information for each document in the dataset\n",
    "document_ids = []  # To store document IDs\n",
    "document_titles = []  # To store document titles\n",
    "document_authors = []  # To store document authors\n",
    "document_fulltexts = []  # To store the full text of each document\n",
    "\n",
    "# Loop through each document in the dataset, as read by the `dataset_reader` function from the constellate library\n",
    "for document in dataset_reader(dataset_file):\n",
    "    # For each document, extract and append its ID, title, author(s), and full text to their respective lists\n",
    "    document_ids.append(document.get('id'))  # Extract and append the document's ID\n",
    "    document_titles.append(document.get('title'))  # Extract and append the document's title\n",
    "    document_authors.append(document.get('creator'))  # Extract and append the document's author(s)\n",
    "    document_fulltexts.append(document.get('fullText'))  # Extract and append the document's full text\n",
    "\n",
    "# Create a pandas DataFrame from the collected lists, organizing the data into columns\n",
    "df = pd.DataFrame({\n",
    "    'id': document_ids,  # Column for document IDs\n",
    "    'title': document_titles,  # Column for document titles\n",
    "    'author': document_authors,  # Column for document authors\n",
    "    'fullText': document_fulltexts  # Column for document full texts\n",
    "})\n",
    "\n",
    "# Print the first few rows of the DataFrame to get a preview of the data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Large Language Models (LLMs) have revolutionized natural language processing and artificial intelligence. These powerful models, trained on vast amounts of text data, can generate human-like text, answer questions, and perform a wide range of language-related tasks. Understanding LLMs is crucial for researchers, educators, and professionals working with text analysis and AI applications.\n",
    "\n",
    "In this lesson, we will:\n",
    "1. Explore the basic concepts behind LLMs\n",
    "2. Interact with an LLM (GPT-3.5) using the OpenAI API\n",
    "3. Analyze LLM performance on various tasks\n",
    "4. Discuss the strengths and limitations of current LLM technology\n",
    "\n",
    "By the end of this lesson, you will have hands-on experience working with LLMs and a deeper understanding of their capabilities and potential applications in research and education.\n",
    "\n",
    "# Lesson\n",
    "\n",
    "## 1. Understanding Large Language Models\n",
    "\n",
    "Large Language Models are deep learning models trained on massive amounts of text data. They use transformer architecture and self-attention mechanisms to process and generate text. Some key concepts:\n",
    "\n",
    "- Transformer architecture\n",
    "- Self-attention mechanism\n",
    "- Token-based processing\n",
    "- Fine-tuning and few-shot learning\n",
    "\n",
    "Let's start by setting up our OpenAI API access:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996cb30a",
   "metadata": {},
   "source": [
    "## Configure the OpenAI client\n",
    "\n",
    "To setup the client for our use, we need to create an API key to use with our request. Skip these steps if you already have an API key for usage.\n",
    "\n",
    "You can get an API key by following these steps:\n",
    "\n",
    "1. [Create a new project](https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects)\n",
    "2. [Generate an API key in your project](https://platform.openai.com/api-keys)\n",
    "3. (RECOMMENDED, BUT NOT REQUIRED) [Setup your API key for all projects as an env var](https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d06be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1: Directly paste the API key (not recommended for production or shared code)\n",
    "client = OpenAI(api_key=\"your_actual_openai_api_key_here\")\n",
    "\n",
    "# Method 2: Use an environment variable (recommended for most use cases)\n",
    "# Ensure the environment variable OPENAI_API_KEY is set in your environment before running the script\n",
    "#client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Method 3: Use a configuration file (alternative for keeping keys out of code)\n",
    "# Create a file named config.py (or similar) and define OPENAI_API_KEY in it, then import it here\n",
    "#from config import OPENAI_API_KEY\n",
    "#client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Method 4: Use Python's built-in `getpass` module to securely input the API key at runtime (useful for notebooks or temporary scripts)\n",
    "#from getpass import getpass\n",
    "#api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "#client = OpenAI(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function `get_completion` that takes a prompt and optionally a model name (defaulting to \"gpt-3.5-turbo\")\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    # Create a list of messages where each message is a dictionary with a role (user/system) and the content (the prompt)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # Call the OpenAI API's chat.completions.create method with the specified model, messages, and a temperature of 0\n",
    "    # Temperature of 0 makes the model's responses deterministic (the same input will always produce the same output)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    # Return the content of the first message in the response's choices. This is the model's completion of the input prompt.\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test the function by defining a test prompt asking to explain what a large language model is in one sentence\n",
    "test_prompt = \"Explain what a large language model is in one sentence.\"\n",
    "# Print the result of calling `get_completion` with the test prompt to see the model's response\n",
    "print(get_completion(test_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0042720",
   "metadata": {},
   "source": [
    "## 2. Basic Interaction with LLMs\n",
    "\n",
    "Now that we have our API set up, let's explore some basic interactions with the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a24518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list named `prompts` containing three different prompts for the language model\n",
    "prompts = [\n",
    "    \"Summarize the plot of Pride and Prejudice in 3 sentences.\",  # First prompt\n",
    "    \"What are the main themes in Frankenstein?\",  # Second prompt\n",
    "    \"Describe Alice's character in Alice in Wonderland.\",  # Third prompt\n",
    "]\n",
    "\n",
    "# Iterate over each prompt in the `prompts` list\n",
    "for prompt in prompts:\n",
    "    # Print the current prompt to the console, formatted with a prefix \"Prompt: \"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    # Call the `get_completion` function with the current prompt, print the response prefixed with \"Response: \"\n",
    "    # The `get_completion` function is expected to return a string containing the model's response to the prompt\n",
    "    print(f\"Response: {get_completion(prompt)}\\n\")  # A newline is added after each response for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce790d",
   "metadata": {},
   "source": [
    "## 3. Analyzing LLM Performance\n",
    "\n",
    "Let's analyze the LLM's performance on various literary analysis tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the number of tokens in a string using a specific encoding\n",
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    # Retrieve the encoding object for the specified encoding name using tiktoken library\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    # Encode the string and calculate the number of tokens in the encoded result\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    # Return the number of tokens\n",
    "    return num_tokens\n",
    "\n",
    "# Define a function to analyze a text in parts, each part having a maximum number of tokens\n",
    "def analyze_text_in_parts(text, task, max_tokens=8000):\n",
    "    # Check if the input text is a list (of strings) or a single string\n",
    "    if isinstance(text, list):\n",
    "        # If it's a list, join the elements into a single string separated by spaces\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    # Initialize an empty list to hold the parts of text\n",
    "    parts = []\n",
    "    # Initialize an empty string to accumulate the current part of text\n",
    "    current_part = \"\"\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Iterate over each word in the text\n",
    "    for word in words:\n",
    "        # Check if adding the current word to the current part exceeds the max_tokens limit\n",
    "        if num_tokens_from_string(current_part + \" \" + word) > max_tokens:\n",
    "            # If so, add the current part to the parts list and start a new part with the current word\n",
    "            parts.append(current_part)\n",
    "            current_part = word\n",
    "        else:\n",
    "            # Otherwise, add the current word to the current part\n",
    "            current_part += \" \" + word\n",
    "    \n",
    "    # After iterating through all words, add the last part to the parts list if it's not empty\n",
    "    if current_part:\n",
    "        parts.append(current_part)\n",
    "    \n",
    "    # Initialize an empty string to accumulate the aggregated response from analyzing each part\n",
    "    aggregated_response = \"\"\n",
    "    # Iterate over each part\n",
    "    for part in parts:\n",
    "        # Construct a prompt for the analysis task using the current part\n",
    "        prompt = f\"Analyze the following text for {task}: {part}\"\n",
    "        # Get the response for the prompt using the get_completion function\n",
    "        response = get_completion(prompt)\n",
    "        # Append the response to the aggregated response, separated by spaces\n",
    "        aggregated_response += response + \" \"\n",
    "        # Sleep for 1 second to avoid hitting rate limits of the API\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Return the aggregated response, stripped of leading/trailing whitespace\n",
    "    return aggregated_response.strip()\n",
    "\n",
    "# Define a list of tasks for analysis\n",
    "tasks = [\"main themes\", \"writing style\", \"historical context\"]\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over each row in the dataframe `df`\n",
    "for _, row in df.iterrows():\n",
    "    # For each task, analyze the full text of the row\n",
    "    for task in tasks:\n",
    "        analysis = analyze_text_in_parts(row['fullText'], task)\n",
    "        # Append the analysis result to the results list\n",
    "        results.append({\"Text\": row['title'], \"Task\": task, \"Analysis\": analysis})\n",
    "    \n",
    "    # After analyzing all tasks for a row, save the results to a CSV file to prevent data loss\n",
    "    pd.DataFrame(results).to_csv('day1_dataset_analysis.csv', index=False)\n",
    "\n",
    "# Convert the results list to a DataFrame and print it\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cce476",
   "metadata": {},
   "source": [
    "## 4. Visualizing LLM Performance\n",
    "\n",
    "Let's create a simple visualization to compare the length of LLM responses for different tasks and texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cfd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'ResponseLength' to the dataframe 'df_results' that contains the length of each response in the 'Analysis' column\n",
    "df_results['ResponseLength'] = df_results['Analysis'].str.len()\n",
    "\n",
    "# Create a new figure for plotting with a specified size (12 inches wide by 6 inches tall)\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Pivot the dataframe to have 'Text' as the index, 'Task' as the columns, and 'ResponseLength' as the values, then plot a bar chart\n",
    "df_results.pivot(index='Text', columns='Task', values='ResponseLength').plot(kind='bar')\n",
    "# Set the title of the plot to 'LLM Response Length by Text and Task'\n",
    "plt.title('LLM Response Length by Text and Task')\n",
    "# Label the x-axis as 'Text'\n",
    "plt.xlabel('Text')\n",
    "# Label the y-axis as 'Response Length (characters)'\n",
    "plt.ylabel('Response Length (characters)')\n",
    "# Add a legend to the plot with the title 'Task'\n",
    "plt.legend(title='Task')\n",
    "# Adjust the layout to make sure everything fits without overlapping\n",
    "plt.tight_layout()\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81f303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initial data to csv for next class\n",
    "df.to_csv('day1_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de74f6",
   "metadata": {},
   "source": [
    "## 5. Discussing Strengths and Limitations\n",
    "\n",
    "Based on our experiments, let's discuss some strengths and limitations of LLMs:\n",
    "\n",
    "Strengths:\n",
    "1. Versatility in handling different types of questions and tasks\n",
    "2. Ability to generate coherent and contextually relevant responses\n",
    "3. Fast processing of large amounts of text\n",
    "\n",
    "Limitations:\n",
    "1. Potential for factual inaccuracies or \"hallucinations\"\n",
    "2. Limited context window for processing long texts\n",
    "3. Difficulty with tasks requiring deep reasoning or external knowledge\n",
    "\n",
    "# Exercises\n",
    "\n",
    "1. Choose a short passage (about 500 words) from one of the downloaded texts and ask the LLM to perform the following tasks:\n",
    "   a. Summarize the passage\n",
    "   b. Identify the mood or tone\n",
    "   c. List any literary devices used\n",
    "\n",
    "2. Compare the LLM's analysis with your own interpretation. What similarities and differences do you notice?\n",
    "\n",
    "3. Experiment with different prompting techniques (e.g., few-shot learning, chain-of-thought) to improve the LLM's performance on a specific task of your choice.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this lesson, we've explored the basics of Large Language Models, interacted with an LLM using the OpenAI API, and analyzed its performance on various literary analysis tasks. We've seen both the impressive capabilities and some limitations of current LLM technology.\n",
    "\n",
    "In the next lesson, we'll dive into embeddings and introduce the concept of Retrieval Augmented Generation (RAG) to enhance LLM performance.\n",
    "\n",
    "# References\n",
    "\n",
    "1. Vaswani, A., et al. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762). arXiv preprint arXiv:1706.03762.\n",
    "2. Brown, T. B., et al. (2020). [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165). arXiv preprint arXiv:2005.14165.\n",
    "3. OpenAI. (2023). [OpenAI API Documentation](https://platform.openai.com/docs/introduction).\n",
    "\n",
    "___\n",
    "[Proceed to next lesson: LLMs with RAG Workshop: Day 2 - Exploring Embeddings and Introduction to RAG ->](./rag_embedding_basics.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to calculate the number of tokens in a string using a specific encoding\n",
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    # Retrieve the encoding object for the specified encoding name using tiktoken library\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    # Encode the string and calculate the number of tokens in the encoded result\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    # Return the number of tokens\n",
    "    return num_tokens\n",
    "\n",
    "# Define a function to analyze a text in parts, each part having a maximum number of tokens\n",
    "def analyze_text_in_parts(text, task, max_tokens=500):\n",
    "    # Check if the input text is a list (of strings) or a single string\n",
    "    if isinstance(text, list):\n",
    "        # If it's a list, join the elements into a single string separated by spaces\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    # Initialize an empty list to hold the parts of text\n",
    "    parts = []\n",
    "    # Initialize an empty string to accumulate the current part of text\n",
    "    current_part = \"\"\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Iterate over each word in the text\n",
    "    for word in words:\n",
    "        # Check if adding the current word to the current part exceeds the max_tokens limit\n",
    "        if num_tokens_from_string(current_part + \" \" + word) > max_tokens:\n",
    "            # If so, add the current part to the parts list and start a new part with the current word\n",
    "            parts.append(current_part)\n",
    "            current_part = word\n",
    "        else:\n",
    "            # Otherwise, add the current word to the current part\n",
    "            current_part += \" \" + word\n",
    "    \n",
    "    # After iterating through all words, add the last part to the parts list if it's not empty\n",
    "    if current_part:\n",
    "        parts.append(current_part)\n",
    "    \n",
    "    # Initialize an empty string to accumulate the aggregated response from analyzing each part\n",
    "    aggregated_response = \"\"\n",
    "    # Iterate over each part\n",
    "    for part in parts:\n",
    "        # Construct a prompt for the analysis task using the current part\n",
    "        prompt = f\"Analyze the following text for {task}: {part}\"\n",
    "        # Assuming get_completion is a function that sends the prompt to an LLM and returns the response\n",
    "        response = get_completion(prompt)  # Implement get_completion according to your LLM's API\n",
    "        # Append the response to the aggregated response, separated by spaces\n",
    "        aggregated_response += response + \" \"\n",
    "        # Sleep for 1 second to avoid hitting rate limits of the API\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Return the aggregated response, stripped of leading/trailing whitespace\n",
    "    return aggregated_response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def analyze_text_in_parts(text, task, max_tokens=8000, technique=\"few-shot\"):\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    parts = []\n",
    "    current_part = \"\"\n",
    "    words = text.split()\n",
    "    \n",
    "    for word in words:\n",
    "        if num_tokens_from_string(current_part + \" \" + word) > max_tokens:\n",
    "            parts.append(current_part)\n",
    "            current_part = word\n",
    "        else:\n",
    "            current_part += \" \" + word\n",
    "    \n",
    "    if current_part:\n",
    "        parts.append(current_part)\n",
    "    \n",
    "    aggregated_response = \"\"\n",
    "    for part in parts:\n",
    "        if technique == \"few-shot\":\n",
    "            prompt = few_shot_prompt(task, part)\n",
    "        elif technique == \"chain-of-thought\":\n",
    "            prompt = chain_of_thought_prompt(task, part)\n",
    "        else:\n",
    "            prompt = f\"Analyze the following text for {task}: {part}\"\n",
    "        \n",
    "        response = get_completion(prompt)  # Implement get_completion according to your LLM's API\n",
    "        aggregated_response += response + \" \"\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return aggregated_response.strip()\n",
    "\n",
    "def few_shot_prompt(task, text):\n",
    "    # Example of a few-shot prompt with two examples\n",
    "    return f\"\"\"\n",
    "    Task: {task}\n",
    "    Example 1: [The text is about the main themes.....]\n",
    "    Example 2: [The historical context of the text is.....]\n",
    "    Text: {text}\n",
    "    Analysis:\n",
    "    \"\"\"\n",
    "\n",
    "def chain_of_thought_prompt(task, text):\n",
    "    # Example of a chain-of-thought prompt\n",
    "    return f\"\"\"\n",
    "    Task: {task}\n",
    "    To analyze the text, consider the following steps:\n",
    "    1. Identify the key themes.\n",
    "    2. Note the writing style and tone.\n",
    "    3. Consider the historical context or relevance.\n",
    "    Text: {text}\n",
    "    Analysis:\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d2e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", frequency_penalty=0):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        frequency_penalty=frequency_penalty,  # Add frequency penalty here\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd12d4",
   "metadata": {},
   "source": [
    "Effects of Changing the Frequency Penalty:\n",
    "\n",
    "Higher Frequency Penalty (>0): Makes the model less likely to repeat the same text verbatim. This can be useful for generating more diverse and creative responses. However, setting it too high might lead to less coherent or relevant responses.\n",
    "\n",
    "Lower Frequency Penalty (closer to 0): Allows more repetition. This can be useful when you want the model to focus on a specific topic or when repetition is not a concern. However, too low might result in responses that are too repetitive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
