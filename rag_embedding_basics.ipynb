{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Grant Glass](https://glassgrant.com) for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email grantg@unc.edu.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Large Language Models and Embeddings for Retrieval Augmented Generation: Day 2 7/17/24\n",
    "\n",
    "This is lesson `2` of 3 in the educational series on `Large Language Models (LLMs) and Retrieval Augmented Generation (RAG)`. This notebook focuses on understanding embeddings and introducing the concept of RAG.\n",
    "\n",
    "**Skills:** \n",
    "* Data analysis\n",
    "* Machine learning\n",
    "* Text analysis\n",
    "* Language models\n",
    "* Vector embeddings\n",
    "* Retrieval Augmented Generation\n",
    "\n",
    "**Audience:** `Learners`\n",
    "\n",
    "**Use case:** `Tutorial`\n",
    "\n",
    "This tutorial guides users through the process of creating and using embeddings, and introduces the concept of Retrieval Augmented Generation.\n",
    "\n",
    "\n",
    "\n",
    "**Difficulty:** `Intermediate`\n",
    "\n",
    "Intermediate assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.\n",
    "\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "* Basic understanding of machine learning concepts\n",
    "* Familiarity with LLMs (covered in Day 1)\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "* Experience with natural language processing (NLP) concepts\n",
    "* Familiarity with vector operations\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "1. Explain the concept of embeddings and their role in NLP\n",
    "2. Generate and visualize embeddings using pre-trained models\n",
    "3. Implement basic similarity search using embeddings\n",
    "4. Describe the principles of Retrieval Augmented Generation (RAG)\n",
    "5. Develop a simple RAG system using embeddings and an LLM\n",
    "\n",
    "**Research Pipeline:**\n",
    "1. Introduction to LLMs and their applications (Day 1)\n",
    "2. **Exploring embeddings and introduction to RAG**\n",
    "3. Optimizing RAG systems (Day 3)\n",
    "4. Applying RAG in research contexts\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "\n",
    "* [OpenAI](https://github.com/openai/openai-python) for generating embeddings and interacting with GPT models\n",
    "* [Pandas](https://pandas.pydata.org/) for data manipulation\n",
    "* [NumPy](https://numpy.org/) for numerical operations\n",
    "* [Matplotlib](https://matplotlib.org/) for data visualization\n",
    "* [Scikit-learn](https://scikit-learn.org/) for dimensionality reduction and similarity calculations\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install Libraries ###\n",
    "!pip install openai pandas numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "\n",
    "# Import the openai library for accessing OpenAI's API functionalities\n",
    "import openai\n",
    "# Import the OpenAI class from the openai library for direct use of its methods (though this seems redundant given the previous import)\n",
    "from openai import OpenAI\n",
    "# Import pandas, a powerful data manipulation and analysis library, and use 'pd' as its alias\n",
    "import pandas as pd\n",
    "# Import numpy, a library for numerical operations on large, multi-dimensional arrays and matrices, using 'np' as its alias\n",
    "import numpy as np\n",
    "# Import pyplot from matplotlib, a plotting library, and use 'plt' as its alias for creating static, interactive, and animated visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# Import TSNE from sklearn.manifold, a tool for dimensionality reduction suitable for visualization of high-dimensional datasets\n",
    "from sklearn.manifold import TSNE\n",
    "# Import cosine_similarity from sklearn.metrics.pairwise, a method to compute similarity between pairs of elements using cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Import the os module, a standard library module providing a way to use operating system dependent functionality like reading or writing to the file system\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedd148",
   "metadata": {},
   "source": [
    "# Required Data\n",
    "\n",
    "We'll continue using the texts from Day 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9da404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('day1_dataset.csv')  # Assuming we saved the DataFrame from Day 1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49845026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the words and make a new column\n",
    "df['word_count'] = df['fullText'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e89f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique words and make a new column\n",
    "df['unique_word_count'] = df['fullText'].apply(lambda x: len(set(str(x).split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d032232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the unqiue topics and make a new column\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in df['fullText']]  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "corpus = [dictionary.doc2bow(text) for text in doc_clean]\n",
    "\n",
    "# Apply LDA\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=50)\n",
    "\n",
    "# Function to count significant topics for a document\n",
    "def count_significant_topics(doc_bow, threshold=0.1):\n",
    "    topic_distribution = ldamodel.get_document_topics(doc_bow)\n",
    "    # Count topics with a distribution above the threshold\n",
    "    return sum(1 for _, prob in topic_distribution if prob > threshold)\n",
    "\n",
    "# Apply the function to each document in the corpus\n",
    "df['significant_topics'] = [count_significant_topics(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab81d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lesson, we'll explore the concept of embeddings, which are dense vector representations of text that capture semantic meaning. We'll then use these embeddings to implement a basic Retrieval Augmented Generation (RAG) system, which combines the power of LLMs with the ability to retrieve relevant information from a knowledge base.\n",
    "\n",
    "Key topics we'll cover:\n",
    "1. Understanding and generating embeddings\n",
    "2. Visualizing embeddings\n",
    "3. Implementing similarity search using embeddings\n",
    "4. Introduction to Retrieval Augmented Generation (RAG)\n",
    "5. Building a simple RAG system\n",
    "\n",
    "Let's begin by setting up our OpenAI API access:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996cb30a",
   "metadata": {},
   "source": [
    "## Configure the OpenAI client\n",
    "\n",
    "To setup the client for our use, we need to create an API key to use with our request. Skip these steps if you already have an API key for usage.\n",
    "\n",
    "You can get an API key by following these steps:\n",
    "\n",
    "1. [Create a new project](https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects)\n",
    "2. [Generate an API key in your project](https://platform.openai.com/api-keys)\n",
    "3. (RECOMMENDED, BUT NOT REQUIRED) [Setup your API key for all projects as an env var](https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d06be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1: Directly paste the API key (not recommended for production or shared code)\n",
    "client = OpenAI(api_key=\"your_actual_openai_api_key_here\")\n",
    "\n",
    "# Method 2: Use an environment variable (recommended for most use cases)\n",
    "# Ensure the environment variable OPENAI_API_KEY is set in your environment before running the script\n",
    "#client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Method 3: Use a configuration file (alternative for keeping keys out of code)\n",
    "# Create a file named config.py (or similar) and define OPENAI_API_KEY in it, then import it here\n",
    "#from config import OPENAI_API_KEY\n",
    "#client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Method 4: Use Python's built-in `getpass` module to securely input the API key at runtime (useful for notebooks or temporary scripts)\n",
    "#from getpass import getpass\n",
    "#api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "#client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec18e12",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Text Embedding Models Overview\n",
    "\n",
    "Text embedding models are designed to convert text into numerical representations, making it easier for machines to understand, compare, and process natural language. Here are some of the models available:\n",
    "\n",
    "### 1. [`text-embedding-ada-002`](command:_github.copilot.openSymbolFromReferences?%5B%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22%2FUsers%2Fgrantglass%2Fteaching%2Fconstellate-RAG-workshop%2Frag_embedding_basics.ipynb%22%2C%22external%22%3A%22vscode-notebook-cell%3A%2FUsers%2Fgrantglass%2Fteaching%2Fconstellate-RAG-workshop%2Frag_embedding_basics.ipynb%23X13sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2FUsers%2Fgrantglass%2Fteaching%2Fconstellate-RAG-workshop%2Frag_embedding_basics.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X13sZmlsZQ%3D%3D%22%7D%2C%7B%22line%22%3A1%2C%22character%22%3A18%7D%5D \"/Users/grantglass/teaching/constellate-RAG-workshop/rag_embedding_basics.ipynb\")\n",
    "- **Description**: A smaller, more efficient model suitable for tasks that require less precision and can benefit from faster processing times. It's part of the GPT-3 family but optimized for embedding tasks.\n",
    "- **Use Cases**: Quick prototyping, mobile applications, or any scenario where speed is more critical than the absolute precision of embeddings.\n",
    "\n",
    "### 2. [`text-embedding-babbage-001`](command:_github.copilot.openSymbolFromReferences?%5B%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22%2FUsers%2Fgrantglass%2Fteaching%2Fconstellate-RAG-workshop%2Frag_embedding_basics.ipynb%22%2C%22external%22%3A%22vscode-notebook-cell%3A%2FUsers%2Fgrantglass%2Fteaching%2Fconstellate-RAG-workshop%2Frag_embedding_basics.ipynb%23X13sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2FUsers%2Fgrantglass%2Fteaching%2Fconstellate-RAG-workshop%2Frag_embedding_basics.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X13sZmlsZQ%3D%3D%22%7D%2C%7B%22line%22%3A1%2C%22character%22%3A18%7D%5D \"/Users/grantglass/teaching/constellate-RAG-workshop/rag_embedding_basics.ipynb\")\n",
    "- **Description**: A mid-range model that offers a balance between performance and computational efficiency. It provides more detailed embeddings than `ada` and is suitable for a wide range of applications.\n",
    "- **Use Cases**: General-purpose embedding tasks where there is a need for a balance between precision and computational resources.\n",
    "\n",
    "### 3. [`text-embedding-curie-001`](command:_github.copilot.openSymbolFromReferences?%5B%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22%2FUsers%2Fgrantglass%2Fteaching%2Fconstellate-RAG-workshop%2Frag_embedding_basics.ipynb%22%2C%22external%22%3A%22vscode-notebook-cell%3A%2FUsers%2Fgrantglass%2Fteaching%2Fconstellate-RAG-workshop%2Frag_embedding_basics.ipynb%23X13sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2FUsers%2Fgrantglass%2Fteaching%2Fconstellate-RAG-workshop%2Frag_embedding_basics.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X13sZmlsZQ%3D%3D%22%7D%2C%7B%22line%22%3A1%2C%22character%22%3A18%7D%5D \"/Users/grantglass/teaching/constellate-RAG-workshop/rag_embedding_basics.ipynb\")\n",
    "- **Description**: A high-performance model that generates more nuanced and detailed embeddings. It's significantly larger than `ada` and `babbage`, making it more computationally intensive.\n",
    "- **Use Cases**: Complex natural language understanding tasks, such as sentiment analysis, summarization, or when the highest quality embeddings are required.\n",
    "\n",
    "### 4. [`text-embedding-davinci-002`](command:_github.copilot.openSymbolFromReferences?%5B%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22%2FUsers%2Fgrantglass%2Fteaching%2Fconstellate-RAG-workshop%2Frag_embedding_basics.ipynb%22%2C%22external%22%3A%22vscode-notebook-cell%3A%2FUsers%2Fgrantglass%2Fteaching%2Fconstellate-RAG-workshop%2Frag_embedding_basics.ipynb%23X13sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2FUsers%2Fgrantglass%2Fteaching%2Fconstellate-RAG-workshop%2Frag_embedding_basics.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X13sZmlsZQ%3D%3D%22%7D%2C%7B%22line%22%3A1%2C%22character%22%3A18%7D%5D \"/Users/grantglass/teaching/constellate-RAG-workshop/rag_embedding_basics.ipynb\")\n",
    "- **Description**: The most advanced and largest model in the series, offering the highest quality embeddings with a deep understanding of context and nuances in the text.\n",
    "- **Use Cases**: High-stakes applications where the quality of the embeddings directly impacts the outcome, such as legal document analysis, medical research, and advanced natural language understanding tasks.\n",
    "\n",
    "### Choosing the Right Model\n",
    "Selecting the right model depends on your specific needs:\n",
    "- **For rapid development and lower resource consumption**: Consider `ada`.\n",
    "- **For a balance between performance and efficiency**: `babbage` is a good choice.\n",
    "- **When quality cannot be compromised**: Opt for `curie` or `davinci`, depending on the level of sophistication and resource availability you have.\n",
    "\n",
    "Each model has its strengths and is optimized for different scenarios, so the choice should be based on the specific requirements of your application, including factors like computational resources, the complexity of the task, and the need for precision in the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the embedding of a given text using a specified model\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    # Replace newline characters with spaces in the text to ensure it's on a single line\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Use the OpenAI API client to create an embedding for the text using the specified model\n",
    "    # The function returns the embedding of the first (and only) input text\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "# Define a function to get a completion for a given prompt using a specified model\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    # Prepare the prompt as a message from the user\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # Use the OpenAI API client to create a chat completion using the specified model\n",
    "    # The temperature parameter is set to 0 for deterministic, less random responses\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    # Return the content of the first (and only) completion message\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Print a message indicating that the OpenAI API client is ready for use\n",
    "print(\"OpenAI API is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0042720",
   "metadata": {},
   "source": [
    "# Lesson\n",
    "\n",
    "## 1. Understanding and Generating Embeddings\n",
    "\n",
    "Embeddings are vector representations of text that capture semantic meaning. Let's generate embeddings for a sample of our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a24518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each row in the dataframe by applying the get_embedding function\n",
    "# The get_embedding function is applied to the first 1000 characters of the 'fullText' column for each row\n",
    "# This is done for brevity and to ensure the embedding process is manageable and efficient\n",
    "df['embedding'] = df['fullText'].apply(lambda x: get_embedding(x[:1000]))\n",
    "\n",
    "# Print the first 5 rows of the dataframe showing only the 'title' and 'embedding' columns\n",
    "# This provides a quick overview of the embeddings generated for the initial texts\n",
    "print(df[['title', 'embedding']].head())\n",
    "\n",
    "# Save the dataframe to a CSV file named 'day2_dataset.csv'\n",
    "# The index=False parameter is used to indicate that the dataframe index should not be written to the file\n",
    "# This results in a CSV file that contains only the data columns\n",
    "df.to_csv('day2_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0f50c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Explanation of the Embedding Code\n",
    "\n",
    "\n",
    "1. **DataFrame Column Access**: `df['fullText']` accesses the `fullText` column of the DataFrame `df`. This column is expected to contain text data for which embeddings will be generated.\n",
    "\n",
    "2. **Apply Function**: The `.apply()` method is used to apply a function along the axis of the DataFrame. In this case, it's applied to each element (row) of the `fullText` column.\n",
    "\n",
    "3. **Lambda Function**: A lambda function is defined inline to process each value `x` (representing the text in each row of the `fullText` column). The lambda function is used here for its conciseness and the ability to define a function in a single line.\n",
    "\n",
    "4. **Text Truncation**: Inside the lambda function, `x[:1000]` truncates the text to the first 1000 characters. This step is likely taken to ensure consistent embedding sizes, reduce computational load, or comply with the limitations of the `get_embedding` function.\n",
    "\n",
    "5. **Embedding Generation**: `get_embedding(x[:1000])` calls a function named `get_embedding`, passing the truncated text as an argument. This function is responsible for converting the text into a numerical representation, known as an embedding. The specifics of how `get_embedding` works, including the model or algorithm it uses, are not provided in the snippet.\n",
    "\n",
    "6. **Storing Embeddings**: The result of the `get_embedding` function, which is the embedding for each row's text, is then stored in a new column in the DataFrame named `embedding`. This effectively adds a new column to `df`, where each row contains the embedding of the text from the `fullText` column.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda6dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could look at different ways of chunking up the text for processing\n",
    "#import textwrap\n",
    "#from typing import List\n",
    "\n",
    "#def get_chunk_embeddings(text: str, chunk_size: int = 1000) -> List:\n",
    "    # Split the text into chunks without splitting words\n",
    "    #chunks = textwrap.wrap(text, width=chunk_size, break_long_words=False)\n",
    "    \n",
    "    # Get embedding for each chunk\n",
    "   # embeddings = [get_embedding(chunk) for chunk in chunks]\n",
    "    \n",
    "    #return embeddings\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "#df['embeddings'] = df['fullText'].apply(lambda x: get_chunk_embeddings(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce790d",
   "metadata": {},
   "source": [
    "## 2. Visualizing Embeddings\n",
    "\n",
    "To visualize high-dimensional embeddings, we'll use t-SNE to reduce them to 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of embeddings from the dataframe into a 2D numpy array for numerical operations\n",
    "embeddings_array = np.array(df['embedding'].tolist())\n",
    "\n",
    "# Determine the perplexity value for t-SNE, ensuring it's less than the number of samples to avoid errors\n",
    "# The perplexity value influences how t-SNE balances attention between local and global aspects of your data\n",
    "# The minimum function ensures the perplexity is not greater than the number of samples minus one\n",
    "perplexity_value = min(30, len(embeddings_array) - 1)\n",
    "\n",
    "# Initialize the t-SNE model with two components (for 2D visualization), a fixed random state for reproducibility,\n",
    "# and the calculated perplexity value\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)\n",
    "\n",
    "# Fit the t-SNE model on the embeddings array to reduce its dimensionality to two dimensions\n",
    "embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "# Create a figure for plotting with a specified size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Scatter plot of the two-dimensional embeddings\n",
    "# Each point represents an embedding, plotted according to its t-SNE reduced dimensions\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "\n",
    "# Annotate each point in the scatter plot with its corresponding title from the dataframe\n",
    "# This loop goes through each embedding and places a text label at its location\n",
    "for i, title in enumerate(df['title']):\n",
    "    plt.annotate(title, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
    "\n",
    "# Set the title of the plot to give context about what is being visualized\n",
    "plt.title('2D Visualization of Embeddings')\n",
    "\n",
    "# Display the plot\n",
    "# This visual representation helps in understanding the relationship between different embeddings\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c721a1b",
   "metadata": {},
   "source": [
    "The point of showing a 2D graph, especially in the context of visualizing embeddings, is to:\n",
    "\n",
    "Simplify Complex Data: Embeddings often exist in high-dimensional spaces that are difficult to visualize or understand intuitively. Reducing these to 2D allows for easy visualization.\n",
    "\n",
    "Discover Patterns: By visualizing embeddings in 2D, one can identify clusters, outliers, or patterns that indicate how the embeddings relate to each other. This can reveal the structure of the data or the effectiveness of the embedding process.\n",
    "\n",
    "Facilitate Analysis: It makes it easier for humans to analyze and interpret the relationships between data points. For instance, in natural language processing (NLP), embeddings that are close together in the 2D space might represent words or documents with similar meanings.\n",
    "\n",
    "Debugging and Improvement: Visualizing the embeddings can help in debugging or improving the model by identifying whether similar items are indeed clustered together as expected.\n",
    "\n",
    "Communication: It provides a straightforward way to communicate complex ideas or relationships in the data to a broader audience, including those without a deep technical background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cce476",
   "metadata": {},
   "source": [
    "## 3. Implementing Similarity Search\n",
    "\n",
    "Let's implement a simple similarity search using cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cfd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the most similar text in a dataframe to a given query\n",
    "def find_most_similar(query, df):\n",
    "    # Generate an embedding for the query text using the get_embedding function\n",
    "    query_embedding = get_embedding(query)\n",
    "    # Calculate the cosine similarity between the query embedding and each embedding in the dataframe\n",
    "    # The similarity score is stored in a new column 'similarity'\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity([query_embedding], [x])[0][0])\n",
    "    # Sort the dataframe by the similarity score in descending order and select the top row\n",
    "    # This row represents the text most similar to the query\n",
    "    return df.sort_values('similarity', ascending=False).iloc[0]\n",
    "\n",
    "# Example usage of the find_most_similar function\n",
    "# Define a query text\n",
    "query = \"A story about a Frederick Douglas\"\n",
    "# Call the find_most_similar function with the query and the dataframe\n",
    "most_similar = find_most_similar(query, df)\n",
    "# Print the results, including the title, summary (full text), and similarity score of the most similar text\n",
    "print(f\"Most similar to '{query}':\")\n",
    "print(f\"Title: {most_similar['title']}\")\n",
    "print(f\"Summary: {most_similar['fullText']}\")\n",
    "print(f\"Similarity score: {most_similar['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027dade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing distance metrics from scipy library\n",
    "from scipy.spatial.distance import euclidean  # Euclidean distance\n",
    "from scipy.spatial.distance import cityblock  # Manhattan (city block) distance\n",
    "\n",
    "# Importing similarity and correlation metrics\n",
    "from sklearn.metrics import jaccard_score  # Jaccard similarity for binary data\n",
    "from scipy.stats import pearsonr  # Pearson correlation coefficient\n",
    "from scipy.stats import spearmanr  # Spearman rank correlation\n",
    "\n",
    "# Importing NLP model and similarity measure from gensim library\n",
    "from gensim.models import Word2Vec  # Word2Vec model for word embeddings\n",
    "from gensim.similarities import WmdSimilarity  # Word Mover's Distance similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e93d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own similairty measure here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de74f6",
   "metadata": {},
   "source": [
    "## 4. Introduction to Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a technique that combines the power of large language models with the ability to retrieve relevant information from a knowledge base. The basic steps of RAG are:\n",
    "\n",
    "1. Convert the query into an embedding\n",
    "2. Find the most similar documents in the knowledge base\n",
    "3. Use the retrieved documents to augment the prompt sent to the LLM\n",
    "\n",
    "Let's implement a simple RAG system using our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8afdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate a response to a query using a Retrieval-Augmented Generation (RAG) approach\n",
    "def rag_response(query, df, model=\"gpt-3.5-turbo\"):\n",
    "    # Generate an embedding for the query using the get_embedding function\n",
    "    query_embedding = get_embedding(query)\n",
    "    # Calculate the cosine similarity between the query embedding and each document's embedding in the dataframe\n",
    "    # Store these similarity scores in a new column 'similarity'\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity([query_embedding], [x])[0][0])\n",
    "    # Find the document with the highest similarity score to the query\n",
    "    most_similar = df.sort_values('similarity', ascending=False).iloc[0]\n",
    "    \n",
    "    # Construct an augmented prompt that includes the context (the most similar document's text, truncated to 1000 characters)\n",
    "    # followed by the query, asking for an informed answer based on the given context\n",
    "    augmented_prompt = f\"\"\"\n",
    "    Given the following context and question, provide an informed answer:\n",
    "    \n",
    "    Context: {most_similar['fullText'][:1000]}...\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the get_completion function to generate a response from the language model (LLM) based on the augmented prompt\n",
    "    return get_completion(augmented_prompt, model)\n",
    "\n",
    "# Example usage of the rag_response function\n",
    "# Define a query regarding the main themes in Frederick Douglass' works\n",
    "query = \"What are the main themes in Frederick Douglass' works?\"\n",
    "# Call the rag_response function with the query and the dataframe containing document embeddings\n",
    "response = rag_response(query, df)\n",
    "# Print the original query and the generated response\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb369a6",
   "metadata": {},
   "source": [
    "## 5. Comparing RAG to Standard LLM Responses\n",
    "\n",
    "Let's compare the RAG response to a standard LLM response without context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c35a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response to the query using a standard Language Model (LLM) without any additional context or augmentation\n",
    "standard_response = get_completion(query)\n",
    "\n",
    "# Print the response generated by the standard LLM\n",
    "print(\"Standard LLM Response:\")\n",
    "print(standard_response)\n",
    "\n",
    "# Print a newline for better readability between the two responses\n",
    "print(\"\\nRAG Response:\")\n",
    "\n",
    "# Print the response generated by the Retrieval-Augmented Generation (RAG) method\n",
    "# This response is expected to be more informed or context-aware due to the use of relevant document(s) during generation\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06818f",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Generate embeddings for longer passages (e.g., first chapters) from the texts we downloaded. Visualize these embeddings and compare them to the smaller embeddings.\n",
    "\n",
    "2. Implement a more sophisticated RAG system that retrieves multiple relevant documents and combines their information in the prompt.\n",
    "\n",
    "3. Experiment with different similarity metrics (e.g., Euclidean distance, Manhattan distance) and compare their performance to cosine similarity.\n",
    "\n",
    "4. Create a simple chatbot that uses the RAG system to answer questions about the texts in our dataset.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this lesson, we've explored the concept of embeddings and how they can be used to capture semantic meaning in text. We've also introduced Retrieval Augmented Generation (RAG) and implemented a basic RAG system using embeddings and an LLM.\n",
    "\n",
    "In the next lesson, we'll focus on optimizing RAG systems for better performance and explore more advanced techniques in this field.\n",
    "\n",
    "# References\n",
    "\n",
    "1. Pennington, J., Socher, R., & Manning, C. D. (2014). [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf). In Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532-1543).\n",
    "2. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805). arXiv preprint arXiv:1810.04805.\n",
    "3. Lewis, P., et al. (2020). [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401). arXiv preprint arXiv:2005.11401.\n",
    "\n",
    "___\n",
    "[Proceed to next lesson: LLMs with RAG Workshop: Day 3 - Optimizing RAG for Enhanced Performance ->](./rag_advanced.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_longer_passage(row, threshold=10000):\n",
    "    \"\"\"\n",
    "    Process a row from the DataFrame to generate embeddings for longer passages.\n",
    "    If the text length is below a certain threshold, it's processed as a single chunk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = row['fullText']\n",
    "        # If the text is longer than the threshold, consider it a longer passage\n",
    "        if len(text) > threshold:\n",
    "            # Process the entire text as a single chunk\n",
    "            return get_embedding(text)\n",
    "        else:\n",
    "            # For shorter texts, use the existing chunking method\n",
    "            return get_chunk_embeddings(text, chunk_size=1000)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply the modified function to each row of the DataFrame\n",
    "df['embeddings_long'] = df.apply(process_longer_passage, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of embeddings from the dataframe into 2D numpy arrays for numerical operations\n",
    "embeddings_short_array = np.array(df['embeddings'].tolist())\n",
    "embeddings_long_array = np.array(df['embeddings_long'].tolist())\n",
    "\n",
    "# Determine the perplexity value for t-SNE, ensuring it's less than the number of samples to avoid errors\n",
    "# Use the smaller of the two arrays to determine the perplexity value\n",
    "perplexity_value = min(30, min(len(embeddings_short_array), len(embeddings_long_array)) - 1)\n",
    "\n",
    "# Initialize the t-SNE model with two components (for 2D visualization), a fixed random state for reproducibility,\n",
    "# and the calculated perplexity value\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)\n",
    "\n",
    "# Fit the t-SNE model on both embeddings arrays to reduce their dimensionality to two dimensions\n",
    "embeddings_short_2d = tsne.fit_transform(embeddings_short_array)\n",
    "embeddings_long_2d = tsne.fit_transform(embeddings_long_array)\n",
    "\n",
    "# Create a figure for plotting with a specified size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Scatter plot of the two-dimensional embeddings for short passages\n",
    "plt.scatter(embeddings_short_2d[:, 0], embeddings_short_2d[:, 1], color='blue', label='Short Passages')\n",
    "\n",
    "# Scatter plot of the two-dimensional embeddings for long passages\n",
    "plt.scatter(embeddings_long_2d[:, 0], embeddings_long_2d[:, 1], color='red', label='Long Passages')\n",
    "\n",
    "# Optionally, annotate points with titles or specific characteristics\n",
    "# This example annotates points from the short passages dataset\n",
    "for i, title in enumerate(df['title']):\n",
    "    plt.annotate(title, (embeddings_short_2d[i, 0], embeddings_short_2d[i, 1]))\n",
    "\n",
    "# Set the title of the plot and add a legend\n",
    "plt.title('2D Visualization of Short vs. Long Passage Embeddings')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826bc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_rag_response(query, df, top_n=3, model=\"gpt-3.5-turbo\"):\n",
    "    # Generate an embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "    # Calculate cosine similarity between the query embedding and each document's embedding\n",
    "    df['similarity'] = df['embeddings'].apply(lambda x: cosine_similarity([query_embedding], [x])[0][0])\n",
    "    # Sort the dataframe by similarity and select the top N most similar documents\n",
    "    top_documents = df.sort_values('similarity', ascending=False).head(top_n)\n",
    "    \n",
    "    # Construct the combined context from the top N documents, truncating each to 1000 characters\n",
    "    combined_context = \"\\n\\n\".join([f\"Context {i+1}: {doc['fullText'][:1000]}...\" for i, doc in top_documents.iterrows()])\n",
    "    \n",
    "    # Construct an augmented prompt with the combined context and the query\n",
    "    augmented_prompt = f\"\"\"\n",
    "    Given the following contexts and question, provide an informed answer:\n",
    "    \n",
    "    {combined_context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a response from the language model based on the augmented prompt\n",
    "    return get_completion(augmented_prompt, model)\n",
    "\n",
    "# Example usage\n",
    "query = \"Defining adaptation studies as it relates to literature into film\"\n",
    "response = improved_rag_response(query, df)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Improved RAG Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming the existence of functions `get_embedding`, `get_completion`, and `improved_rag_response` as defined previously\n",
    "\n",
    "\n",
    "\n",
    "def chatbot():\n",
    "    print(\"Hello! I'm a chatbot that can answer questions about specific adaptation study texts. Ask me anything or type 'quit' to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Use the improved RAG response function to generate an answer\n",
    "        response = improved_rag_response(user_input, df)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "# Example usage\n",
    "chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
