{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Grant Glass](https://glassgrant.com) for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email grantg@unc.edu.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Large Language Models and Embeddings for Retrieval Augmented Generation: Day 2 7/17/24\n",
    "\n",
    "This is lesson `2` of 3 in the educational series on `Large Language Models (LLMs) and Retrieval Augmented Generation (RAG)`. This notebook focuses on understanding embeddings and introducing the concept of RAG.\n",
    "\n",
    "**Skills:** \n",
    "* Data analysis\n",
    "* Machine learning\n",
    "* Text analysis\n",
    "* Language models\n",
    "* Vector embeddings\n",
    "* Retrieval Augmented Generation\n",
    "\n",
    "**Audience:** `Learners`\n",
    "\n",
    "**Use case:** `Tutorial`\n",
    "\n",
    "This tutorial guides users through the process of creating and using embeddings, and introduces the concept of Retrieval Augmented Generation.\n",
    "\n",
    "\n",
    "\n",
    "**Difficulty:** `Intermediate`\n",
    "\n",
    "Intermediate assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.\n",
    "\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "* Basic understanding of machine learning concepts\n",
    "* Familiarity with LLMs (covered in Day 1)\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "* Experience with natural language processing (NLP) concepts\n",
    "* Familiarity with vector operations\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "1. Explain the concept of embeddings and their role in NLP\n",
    "2. Generate and visualize embeddings using pre-trained models\n",
    "3. Implement basic similarity search using embeddings\n",
    "4. Describe the principles of Retrieval Augmented Generation (RAG)\n",
    "5. Develop a simple RAG system using embeddings and an LLM\n",
    "\n",
    "**Research Pipeline:**\n",
    "1. Introduction to LLMs and their applications (Day 1)\n",
    "2. **Exploring embeddings and introduction to RAG**\n",
    "3. Optimizing RAG systems (Day 3)\n",
    "4. Applying RAG in research contexts\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "\n",
    "* [OpenAI](https://github.com/openai/openai-python) for generating embeddings and interacting with GPT models\n",
    "* [Pandas](https://pandas.pydata.org/) for data manipulation\n",
    "* [NumPy](https://numpy.org/) for numerical operations\n",
    "* [Matplotlib](https://matplotlib.org/) for data visualization\n",
    "* [Scikit-learn](https://scikit-learn.org/) for dimensionality reduction and similarity calculations\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install Libraries ###\n",
    "!pip install openai pandas numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "\n",
    "# Import the openai library for accessing OpenAI's API functionalities\n",
    "import openai\n",
    "# Import the OpenAI class from the openai library for direct use of its methods (though this seems redundant given the previous import)\n",
    "from openai import OpenAI\n",
    "# Import pandas, a powerful data manipulation and analysis library, and use 'pd' as its alias\n",
    "import pandas as pd\n",
    "# Import numpy, a library for numerical operations on large, multi-dimensional arrays and matrices, using 'np' as its alias\n",
    "import numpy as np\n",
    "# Import pyplot from matplotlib, a plotting library, and use 'plt' as its alias for creating static, interactive, and animated visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# Import TSNE from sklearn.manifold, a tool for dimensionality reduction suitable for visualization of high-dimensional datasets\n",
    "from sklearn.manifold import TSNE\n",
    "# Import cosine_similarity from sklearn.metrics.pairwise, a method to compute similarity between pairs of elements using cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Import the os module, a standard library module providing a way to use operating system dependent functionality like reading or writing to the file system\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedd148",
   "metadata": {},
   "source": [
    "# Required Data\n",
    "\n",
    "We'll continue using the texts from Day 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9da404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('day1_dataset.csv')  # Assuming we saved the DataFrame from Day 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lesson, we'll explore the concept of embeddings, which are dense vector representations of text that capture semantic meaning. We'll then use these embeddings to implement a basic Retrieval Augmented Generation (RAG) system, which combines the power of LLMs with the ability to retrieve relevant information from a knowledge base.\n",
    "\n",
    "Key topics we'll cover:\n",
    "1. Understanding and generating embeddings\n",
    "2. Visualizing embeddings\n",
    "3. Implementing similarity search using embeddings\n",
    "4. Introduction to Retrieval Augmented Generation (RAG)\n",
    "5. Building a simple RAG system\n",
    "\n",
    "Let's begin by setting up our OpenAI API access:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996cb30a",
   "metadata": {},
   "source": [
    "## Configure the OpenAI client\n",
    "\n",
    "To setup the client for our use, we need to create an API key to use with our request. Skip these steps if you already have an API key for usage.\n",
    "\n",
    "You can get an API key by following these steps:\n",
    "\n",
    "1. [Create a new project](https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects)\n",
    "2. [Generate an API key in your project](https://platform.openai.com/api-keys)\n",
    "3. (RECOMMENDED, BUT NOT REQUIRED) [Setup your API key for all projects as an env var](https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d06be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1: Directly paste the API key (not recommended for production or shared code)\n",
    "client = OpenAI(api_key=\"your_actual_openai_api_key_here\")\n",
    "\n",
    "# Method 2: Use an environment variable (recommended for most use cases)\n",
    "# Ensure the environment variable OPENAI_API_KEY is set in your environment before running the script\n",
    "#client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Method 3: Use a configuration file (alternative for keeping keys out of code)\n",
    "# Create a file named config.py (or similar) and define OPENAI_API_KEY in it, then import it here\n",
    "from config import OPENAI_API_KEY\n",
    "#client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Method 4: Use Python's built-in `getpass` module to securely input the API key at runtime (useful for notebooks or temporary scripts)\n",
    "#from getpass import getpass\n",
    "#api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "#client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the embedding of a given text using a specified model\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    # Replace newline characters with spaces in the text to ensure it's on a single line\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Use the OpenAI API client to create an embedding for the text using the specified model\n",
    "    # The function returns the embedding of the first (and only) input text\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "# Define a function to get a completion for a given prompt using a specified model\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    # Prepare the prompt as a message from the user\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # Use the OpenAI API client to create a chat completion using the specified model\n",
    "    # The temperature parameter is set to 0 for deterministic, less random responses\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    # Return the content of the first (and only) completion message\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Print a message indicating that the OpenAI API client is ready for use\n",
    "print(\"OpenAI API is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0042720",
   "metadata": {},
   "source": [
    "# Lesson\n",
    "\n",
    "## 1. Understanding and Generating Embeddings\n",
    "\n",
    "Embeddings are vector representations of text that capture semantic meaning. Let's generate embeddings for our book summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a24518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each row in the dataframe by applying the get_embedding function\n",
    "# The get_embedding function is applied to the first 1000 characters of the 'fullText' column for each row\n",
    "# This is done for brevity and to ensure the embedding process is manageable and efficient\n",
    "df['embedding'] = df['fullText'].apply(lambda x: get_embedding(x[:1000]))\n",
    "\n",
    "# Print the first 5 rows of the dataframe showing only the 'title' and 'embedding' columns\n",
    "# This provides a quick overview of the embeddings generated for the initial texts\n",
    "print(df[['title', 'embedding']].head())\n",
    "\n",
    "# Save the dataframe to a CSV file named 'day2_dataset.csv'\n",
    "# The index=False parameter is used to indicate that the dataframe index should not be written to the file\n",
    "# This results in a CSV file that contains only the data columns\n",
    "df.to_csv('day2_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce790d",
   "metadata": {},
   "source": [
    "## 2. Visualizing Embeddings\n",
    "\n",
    "To visualize high-dimensional embeddings, we'll use t-SNE to reduce them to 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of embeddings from the dataframe into a 2D numpy array for numerical operations\n",
    "embeddings_array = np.array(df['embedding'].tolist())\n",
    "\n",
    "# Determine the perplexity value for t-SNE, ensuring it's less than the number of samples to avoid errors\n",
    "# The perplexity value influences how t-SNE balances attention between local and global aspects of your data\n",
    "# The minimum function ensures the perplexity is not greater than the number of samples minus one\n",
    "perplexity_value = min(30, len(embeddings_array) - 1)\n",
    "\n",
    "# Initialize the t-SNE model with two components (for 2D visualization), a fixed random state for reproducibility,\n",
    "# and the calculated perplexity value\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)\n",
    "\n",
    "# Fit the t-SNE model on the embeddings array to reduce its dimensionality to two dimensions\n",
    "embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "# Create a figure for plotting with a specified size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Scatter plot of the two-dimensional embeddings\n",
    "# Each point represents an embedding, plotted according to its t-SNE reduced dimensions\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "\n",
    "# Annotate each point in the scatter plot with its corresponding title from the dataframe\n",
    "# This loop goes through each embedding and places a text label at its location\n",
    "for i, title in enumerate(df['title']):\n",
    "    plt.annotate(title, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
    "\n",
    "# Set the title of the plot to give context about what is being visualized\n",
    "plt.title('2D Visualization of Embeddings')\n",
    "\n",
    "# Display the plot\n",
    "# This visual representation helps in understanding the relationship between different embeddings\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cce476",
   "metadata": {},
   "source": [
    "## 3. Implementing Similarity Search\n",
    "\n",
    "Let's implement a simple similarity search using cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cfd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the most similar text in a dataframe to a given query\n",
    "def find_most_similar(query, df):\n",
    "    # Generate an embedding for the query text using the get_embedding function\n",
    "    query_embedding = get_embedding(query)\n",
    "    # Calculate the cosine similarity between the query embedding and each embedding in the dataframe\n",
    "    # The similarity score is stored in a new column 'similarity'\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity([query_embedding], [x])[0][0])\n",
    "    # Sort the dataframe by the similarity score in descending order and select the top row\n",
    "    # This row represents the text most similar to the query\n",
    "    return df.sort_values('similarity', ascending=False).iloc[0]\n",
    "\n",
    "# Example usage of the find_most_similar function\n",
    "# Define a query text\n",
    "query = \"A story about a Frederick Douglas\"\n",
    "# Call the find_most_similar function with the query and the dataframe\n",
    "most_similar = find_most_similar(query, df)\n",
    "# Print the results, including the title, summary (full text), and similarity score of the most similar text\n",
    "print(f\"Most similar to '{query}':\")\n",
    "print(f\"Title: {most_similar['title']}\")\n",
    "print(f\"Summary: {most_similar['fullText']}\")\n",
    "print(f\"Similarity score: {most_similar['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de74f6",
   "metadata": {},
   "source": [
    "## 4. Introduction to Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a technique that combines the power of large language models with the ability to retrieve relevant information from a knowledge base. The basic steps of RAG are:\n",
    "\n",
    "1. Convert the query into an embedding\n",
    "2. Find the most similar documents in the knowledge base\n",
    "3. Use the retrieved documents to augment the prompt sent to the LLM\n",
    "\n",
    "Let's implement a simple RAG system using our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8afdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate a response to a query using a Retrieval-Augmented Generation (RAG) approach\n",
    "def rag_response(query, df, model=\"gpt-3.5-turbo\"):\n",
    "    # Generate an embedding for the query using the get_embedding function\n",
    "    query_embedding = get_embedding(query)\n",
    "    # Calculate the cosine similarity between the query embedding and each document's embedding in the dataframe\n",
    "    # Store these similarity scores in a new column 'similarity'\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity([query_embedding], [x])[0][0])\n",
    "    # Find the document with the highest similarity score to the query\n",
    "    most_similar = df.sort_values('similarity', ascending=False).iloc[0]\n",
    "    \n",
    "    # Construct an augmented prompt that includes the context (the most similar document's text, truncated to 1000 characters)\n",
    "    # followed by the query, asking for an informed answer based on the given context\n",
    "    augmented_prompt = f\"\"\"\n",
    "    Given the following context and question, provide an informed answer:\n",
    "    \n",
    "    Context: {most_similar['fullText'][:1000]}...\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the get_completion function to generate a response from the language model (LLM) based on the augmented prompt\n",
    "    return get_completion(augmented_prompt, model)\n",
    "\n",
    "# Example usage of the rag_response function\n",
    "# Define a query regarding the main themes in Frederick Douglass' works\n",
    "query = \"What are the main themes in Frederick Douglass' works?\"\n",
    "# Call the rag_response function with the query and the dataframe containing document embeddings\n",
    "response = rag_response(query, df)\n",
    "# Print the original query and the generated response\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb369a6",
   "metadata": {},
   "source": [
    "## 5. Comparing RAG to Standard LLM Responses\n",
    "\n",
    "Let's compare the RAG response to a standard LLM response without context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c35a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response to the query using a standard Language Model (LLM) without any additional context or augmentation\n",
    "standard_response = get_completion(query)\n",
    "\n",
    "# Print the response generated by the standard LLM\n",
    "print(\"Standard LLM Response:\")\n",
    "print(standard_response)\n",
    "\n",
    "# Print a newline for better readability between the two responses\n",
    "print(\"\\nRAG Response:\")\n",
    "\n",
    "# Print the response generated by the Retrieval-Augmented Generation (RAG) method\n",
    "# This response is expected to be more informed or context-aware due to the use of relevant document(s) during generation\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06818f",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Generate embeddings for longer passages (e.g., first chapters) from the three books we downloaded. Visualize these embeddings and compare them to the summary embeddings.\n",
    "\n",
    "2. Implement a more sophisticated RAG system that retrieves multiple relevant documents and combines their information in the prompt.\n",
    "\n",
    "3. Experiment with different similarity metrics (e.g., Euclidean distance, Manhattan distance) and compare their performance to cosine similarity.\n",
    "\n",
    "4. Create a simple chatbot that uses the RAG system to answer questions about the books in our dataset.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this lesson, we've explored the concept of embeddings and how they can be used to capture semantic meaning in text. We've also introduced Retrieval Augmented Generation (RAG) and implemented a basic RAG system using embeddings and an LLM.\n",
    "\n",
    "In the next lesson, we'll focus on optimizing RAG systems for better performance and explore more advanced techniques in this field.\n",
    "\n",
    "# References\n",
    "\n",
    "1. Pennington, J., Socher, R., & Manning, C. D. (2014). [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf). In Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532-1543).\n",
    "2. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805). arXiv preprint arXiv:1810.04805.\n",
    "3. Lewis, P., et al. (2020). [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401). arXiv preprint arXiv:2005.11401.\n",
    "\n",
    "___\n",
    "[Proceed to next lesson: LLMs with RAG Workshop: Day 3 - Optimizing RAG for Enhanced Performance ->](./rag_advanced.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
